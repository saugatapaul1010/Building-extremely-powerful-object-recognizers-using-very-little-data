{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras import applications\n",
    "from datetime import datetime as dt\n",
    "from keras import regularizers as reg\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.externals import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to plot the train and test loss after each epoch.\n",
    "import matplotlib.pyplot as plt\n",
    "def plt_loss(x, vy, ty, ax, colors=['b']):\n",
    "    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n",
    "    ax.plot(x, ty, 'r', label=\"Train Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensions of our flicker images is 256 X 256\n",
    "img_width, img_height = 256, 256\n",
    "\n",
    "#Declaration of parameters needed for training and validation\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "epochs = 50\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 5 classes.\n",
      "Found 2500 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "generator_tr = datagen.flow_from_directory(train_data_dir,\n",
    "                                        target_size=(img_width, img_height),\n",
    "                                        batch_size=batch_size,\n",
    "                                        class_mode=None, #class_mode=None means the generator won't load the class labels.\n",
    "                                        shuffle=False) #We won't shuffle the data, because we want the class labels to stay in order.\n",
    "\n",
    "generator_ts = datagen.flow_from_directory(validation_data_dir,\n",
    "                                        target_size=(img_width, img_height),\n",
    "                                        batch_size=batch_size,\n",
    "                                        class_mode=None,\n",
    "                                        shuffle=False)\n",
    "\n",
    "nb_train_samples = len(generator_tr.filenames) #10000. 2000 training samples for each class  \n",
    "nb_validation_samples = len(generator_ts.filenames) #2500. 500 training samples for each class\n",
    "num_classes = len(generator_tr.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"cnn_codes\") if not os.path.isdir(\"cnn_codes\") else None\n",
    "\n",
    "#Get the bottleneck features by  Weights.T * Xi\n",
    "def get_bottleneck_features(model_name):\n",
    "    \n",
    "    start=dt.now()\n",
    "    \n",
    "    #Load the pre trained model from Keras, we will initialize only the convolution layers and ignore the top layers.\n",
    "    if model_name = \"VGG16\":\n",
    "        model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "    elif model_name = \"InceptionV3\":\n",
    "        model = applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "    elif model_name = \"ResNet50\": \n",
    "        model = applications.ResNet50(include_top=False, weights='imagenet')\n",
    "    elif model_name = \"InceptionResNetV2\": \n",
    "        model = applications.InceptionResNetV2(include_top=False, weights='imagenet')\n",
    "    elif model_name = \"DenseNet201\": \n",
    "        model = applications.DenseNet201(include_top=False, weights='imagenet')\n",
    "\n",
    "    bottleneck_features_train = model.predict_generator(generator_tr, nb_train_samples // batch_size)\n",
    "    np.save('cnn_codes/{}_bottleneck_features_train.npy'.format(model_name),bottleneck_features_train) #bottleneck_features_train is a numpy array\n",
    "\n",
    "\n",
    "    bottleneck_features_validation = model.predict_generator(generator_ts, nb_validation_samples // batch_size)\n",
    "    np.save('cnn_codes/{}_bottleneck_features_validation.npy'.format(model_name),bottleneck_features_validation)\n",
    "    print(\"Got the bottleneck features for {} model, in time: \".format(model_name),dt.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VGG16 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Get the bottleneck features using a pretrained VGG16 on Imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 5 classes.\n",
      "Found 2500 images belonging to 5 classes.\n",
      "Got the bottleneck features in time:  1:58:36.778060\n"
     ]
    }
   ],
   "source": [
    "get_bottleneck_features(\"VGG16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train a model with the bottleneck features obtained using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               8388864   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 8,457,989\n",
      "Trainable params: 8,456,965\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 82s 8ms/step - loss: 28.5460 - acc: 0.8111 - val_loss: 14.4443 - val_acc: 0.8756\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.87560, saving model to weights/vgg16_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 79s 8ms/step - loss: 10.7425 - acc: 0.8974 - val_loss: 8.3736 - val_acc: 0.9432\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.87560 to 0.94320, saving model to weights/vgg16_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 80s 8ms/step - loss: 7.2276 - acc: 0.9040 - val_loss: 7.0237 - val_acc: 0.6856\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.94320\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 87s 9ms/step - loss: 5.6585 - acc: 0.9068 - val_loss: 5.2762 - val_acc: 0.9412\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.94320\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 86s 9ms/step - loss: 4.9535 - acc: 0.9034 - val_loss: 4.7884 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.94320\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 87s 9ms/step - loss: 4.3876 - acc: 0.9012 - val_loss: 4.0993 - val_acc: 0.9656\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.94320 to 0.96560, saving model to weights/vgg16_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 89s 9ms/step - loss: 3.9884 - acc: 0.9060 - val_loss: 4.2719 - val_acc: 0.7664\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.96560\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 83s 8ms/step - loss: 3.6988 - acc: 0.8974 - val_loss: 3.8729 - val_acc: 0.8552\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.96560\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 86s 9ms/step - loss: 3.3837 - acc: 0.9023 - val_loss: 3.5157 - val_acc: 0.8312\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.96560\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 78s 8ms/step - loss: 3.1150 - acc: 0.9035 - val_loss: 2.8082 - val_acc: 0.9648\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.96560\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 80s 8ms/step - loss: 2.8341 - acc: 0.9061 - val_loss: 2.6186 - val_acc: 0.9364\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.96560\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 83s 8ms/step - loss: 2.6461 - acc: 0.9047 - val_loss: 2.4298 - val_acc: 0.9516\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.96560\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 87s 9ms/step - loss: 2.4357 - acc: 0.9071 - val_loss: 2.5282 - val_acc: 0.8708\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.96560\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 90s 9ms/step - loss: 2.2589 - acc: 0.9134 - val_loss: 2.1078 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.96560 to 0.97840, saving model to weights/vgg16_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 97s 10ms/step - loss: 2.1685 - acc: 0.9110 - val_loss: 2.1462 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.97840\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 98s 10ms/step - loss: 2.0187 - acc: 0.9128 - val_loss: 2.0014 - val_acc: 0.9056\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.97840\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 97s 10ms/step - loss: 1.9471 - acc: 0.9072 - val_loss: 1.7249 - val_acc: 0.9648\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.97840\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 92s 9ms/step - loss: 1.7877 - acc: 0.9125 - val_loss: 2.2155 - val_acc: 0.8056\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.97840\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 85s 8ms/step - loss: 1.7304 - acc: 0.9103 - val_loss: 1.5170 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.97840\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 90s 9ms/step - loss: 1.6234 - acc: 0.9147 - val_loss: 1.4326 - val_acc: 0.9620\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.97840\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 106s 11ms/step - loss: 1.5663 - acc: 0.9113 - val_loss: 2.4183 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.97840\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 107s 11ms/step - loss: 1.5469 - acc: 0.9057 - val_loss: 1.6381 - val_acc: 0.9092\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.97840\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 95s 10ms/step - loss: 1.4998 - acc: 0.9110 - val_loss: 1.6875 - val_acc: 0.8336\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.97840\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 87s 9ms/step - loss: 1.4274 - acc: 0.9205 - val_loss: 1.2637 - val_acc: 0.9568\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.97840\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 88s 9ms/step - loss: 1.3752 - acc: 0.9189 - val_loss: 1.3016 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.97840\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 83s 8ms/step - loss: 1.3753 - acc: 0.9189 - val_loss: 1.1902 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.97840 to 0.98880, saving model to weights/vgg16_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 92s 9ms/step - loss: 1.3495 - acc: 0.9224 - val_loss: 1.1907 - val_acc: 0.9588\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.98880\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 99s 10ms/step - loss: 1.3041 - acc: 0.9192 - val_loss: 1.3441 - val_acc: 0.9328\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.98880\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 100s 10ms/step - loss: 1.3082 - acc: 0.9193 - val_loss: 1.3320 - val_acc: 0.9268\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.98880\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 100s 10ms/step - loss: 1.2711 - acc: 0.9233 - val_loss: 1.2071 - val_acc: 0.9388\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.98880\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 99s 10ms/step - loss: 1.2452 - acc: 0.9240 - val_loss: 1.1804 - val_acc: 0.9204\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.98880\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 99s 10ms/step - loss: 1.2421 - acc: 0.9236 - val_loss: 1.1316 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.98880 to 0.98960, saving model to weights/vgg16_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 99s 10ms/step - loss: 1.2510 - acc: 0.9261 - val_loss: 1.3164 - val_acc: 0.9096\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.98960\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 97s 10ms/step - loss: 1.3130 - acc: 0.9102 - val_loss: 2.0348 - val_acc: 0.7984\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.98960\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 98s 10ms/step - loss: 1.3189 - acc: 0.9165 - val_loss: 1.3254 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.98960\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 99s 10ms/step - loss: 1.2544 - acc: 0.9257 - val_loss: 2.1580 - val_acc: 0.7988\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.98960\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 99s 10ms/step - loss: 1.2326 - acc: 0.9273 - val_loss: 1.1351 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.98960\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 100s 10ms/step - loss: 1.2529 - acc: 0.9233 - val_loss: 1.1239 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.98960\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 99s 10ms/step - loss: 1.2497 - acc: 0.9245 - val_loss: 1.2852 - val_acc: 0.9460\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.98960\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 95s 10ms/step - loss: 1.2228 - acc: 0.9300 - val_loss: 1.0451 - val_acc: 0.9768\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.98960\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 101s 10ms/step - loss: 1.2628 - acc: 0.9201 - val_loss: 1.1430 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.98960\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 104s 10ms/step - loss: 1.3030 - acc: 0.9197 - val_loss: 1.1682 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.98960\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 103s 10ms/step - loss: 1.2911 - acc: 0.9269 - val_loss: 1.2049 - val_acc: 0.9700\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.98960\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 99s 10ms/step - loss: 1.3230 - acc: 0.9232 - val_loss: 1.6460 - val_acc: 0.7740\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.98960\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 94s 9ms/step - loss: 1.3432 - acc: 0.9150 - val_loss: 1.2968 - val_acc: 0.9480\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.98960\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 92s 9ms/step - loss: 1.3174 - acc: 0.9166 - val_loss: 1.3315 - val_acc: 0.9064\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.98960\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 93s 9ms/step - loss: 1.2661 - acc: 0.9222 - val_loss: 1.3262 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.98960\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 96s 10ms/step - loss: 1.2952 - acc: 0.9169 - val_loss: 1.3933 - val_acc: 0.9072\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.98960\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 112s 11ms/step - loss: 1.3082 - acc: 0.9184 - val_loss: 1.2078 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.98960\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 80s 8ms/step - loss: 1.3404 - acc: 0.9169 - val_loss: 1.3771 - val_acc: 0.8956\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.98960\n",
      "\n",
      "The top layer trained in time:  1:17:43.550363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/vgg16_using_bottleneck_best.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After we get the bottleneck features, we will build the top fully connected layers on top of the bottlneck features. Let's build the top layers.\n",
    "def train_model_vgg16():\n",
    "    global_start=dt.now()\n",
    "\n",
    "    train_data = np.load('cnn_codes/VGG16_bottleneck_features_train.npy')\n",
    "    validation_data = np.load('cnn_codes/VGG16_bottleneck_features_validation.npy')\n",
    "    \n",
    "    #train_labels = np.array([0] * (nb_train_samples // 3) + [1] * (nb_train_samples // 3) + [2] * (nb_train_samples // 3)) #Equivalent to: np.array([0]*1200 + [1]*1200 + [2]*1200)\n",
    "    #validation_labels = np.array([0] * (nb_validation_samples // 3) + [1] * (nb_validation_samples // 3) + [2] * (nb_validation_samples // 3))\n",
    "    train_labels=generator_tr.classes  \n",
    "    validation_labels=generator_ts.classes\n",
    "    \n",
    "    train_labels = to_categorical(train_labels, num_classes=num_classes)  \n",
    "    validation_labels = to_categorical(validation_labels, num_classes=num_classes)  \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:])) #Ignore the first index. It contains ID\n",
    "\n",
    "    model.add(Dense(256, activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001))) #Best weight initializer for relu is he_normal\n",
    "    model.add(BatchNormalization()) #Add a BatchNormalization layer to control internel covariance shift\n",
    "    model.add(Dropout(rate=0.5)) #Using droput for regularization\n",
    "\n",
    "    model.add(Dense(256, activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001)))\n",
    "    model.add(BatchNormalization()) #Add a BatchNormalization layer to control internel covariance shift\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax',kernel_initializer='glorot_uniform')) #Because we have 3 classes. Remember, softmax is to multi-class, what sigmoid (log reg) is to binary\n",
    "\n",
    "    optim=RMSprop(lr=0.0001, epsilon=1e-8, decay=1e-6)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    #Save the weights for the best epoch accuracy\n",
    "    checkpointer = ModelCheckpoint(filepath=\"weights/vgg16_bottleneck_feats_multi_weights.hdf5\", monitor = 'val_acc',verbose=1, save_best_only=True)\n",
    "    model.fit(x=train_data,\n",
    "              y=train_labels,\n",
    "              epochs=epochs,\n",
    "              validation_data=(validation_data, validation_labels),\n",
    "              callbacks=[checkpointer])    \n",
    "    \n",
    "    #Refit our model with the best weights we got using model checkpointer\n",
    "    #This step is absolutely needed, or else we might save our final model with model weights different from those we got using model checkpointer\n",
    "    model.load_weights('weights/vgg16_bottleneck_feats_multi_weights.hdf5')\n",
    "    model.save('models/vgg16_botlnck_trained.h5')\n",
    "    print(\"\\nThe top layer trained in time: \",dt.now()-global_start)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model=train_model_vgg16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Get model performance (VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1316226387023927\n",
      "Validation Accuracy on Unseen Data: 0.9896\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('cnn_codes/VGG16_bottleneck_features_train.npy')\n",
    "validation_data = np.load('cnn_codes/VGG16_bottleneck_features_validation.npy')\n",
    "\n",
    "train_labels = to_categorical(generator_tr.classes, num_classes=num_classes)  \n",
    "validation_labels = to_categorical(generator_ts.classes, num_classes=num_classes)\n",
    "\n",
    "#Plot the train and test loss vs number of epochs\n",
    "score = model.evaluate(validation_data, validation_labels, verbose=0) \n",
    "print('Validation Loss:', score[0]) \n",
    "print('Validation Accuracy on Unseen Data:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualize the train and validation loss for VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl4VOX1wPHvyb4HCBDCIqBBBEIIAQUVIXGhinVrXQqiVlHqWi1qpWp/iktdahWtrXXX1gU3EKsodWERFRQQEVnEBZVFCFtIQhJIcn5/3BtISDJzIZlMJnM+z3Ofmbm59855Q5gz73LfV1QVY4wx4Ssi2AEYY4wJLksExhgT5iwRGGNMmLNEYIwxYc4SgTHGhDlLBMYYE+YsERhjTJizRGCMMWHOEoExxoS5qGAH4EX79u21R48ePo8pKSkhMTGxeQJqQazc4cXKHX4aU/ZFixZtVtUO/o4LiUTQo0cPFi5c6POY2bNnk5eX1zwBtSBW7vBi5Q4/jSm7iPzg5ThrGjLGmDBnicAYY8KcJQJjjAlzIdFHYIwJvN27d7N27VrKysqCHUodqamprFixIthhBIWXssfFxdG1a1eio6MP6D0sERhjAFi7di3Jycn06NEDEQl2OLUUFRWRnJwc7DCCwl/ZVZUtW7awdu1aevbseUDvYU1DxhgAysrKSEtLa3FJwPgmIqSlpTWqJmeJwBizhyWB0NTYf7fWnQjefBPuvjvYURhjTIvWuhPBzJlwzz3BjsIY40FeXh4zZ86stW/y5MlcfvnlPs9LSkoCYP369Zx55pkNXtvfTamTJ09m586de16PGjWK7du3ewndp1tvvZX77ruv0dcJpNadCJKTobgYVIMdiTHGj9GjRzNlypRa+6ZMmcLo0aM9nd+5c2deffXVA37/fRPBjBkzaNOmzQFfL5S07kSQlAQVFVBeHuxIjDF+nHnmmbz55puUu/9f16xZw/r16xk2bBjFxcUcd9xx5Obm0r9/f6ZPn17n/DVr1pCVlQVAaWkpv/nNb8jOzuacc86htLR0z3GXXXYZgwcPpl+/ftxyyy0APPTQQ6xfv578/Hzy8/MBZ2qbzZs3A3D//feTlZVFVlYWkydP3vN+ffr04ZJLLqFfv36MHDmy1vv4U981S0pKOPnkkxkwYABZWVm89NJLAEycOJG+ffuSnZ3Nddddt1+/Vy9a9/DR6iFXxcUQFxfcWIwJIddcA0uWNO01c3LA/byrV1paGkcccQTvvPMOp512GlOmTOGcc85BRIiLi2PatGmkpKSwefNmhg4dyqmnntpgJ+kjjzxCQkICS5cuZenSpeTm5u752Z133km7du2orKzkuOOOY+nSpfz+97/n/vvvZ9asWbRv377WtRYtWsTTTz/NggULUFWGDBnCiBEjaNu2LatXr+bFF1/k8ccf5+yzz+a1115j7Nixfn8XDV3zu+++o3Pnzrz11lsAFBYWsnXrVqZNm8bKlSsRkSZprtpX668RgJMIjDEtXs3moZrNQqrKjTfeSHZ2Nscffzzr1q1j48aNDV5n7ty5ez6Qs7Ozyc7O3vOzl19+mdzcXAYOHMhXX33F8uXLfcY0b948zjjjDBITE0lKSuJXv/oVH374IQA9e/YkJycHgEGDBrFmzRpP5Wzomv379+e9997jhhtu4MMPPyQ1NZWUlBTi4uK4+OKLmTp1KgkJCZ7eY3+07hpBdSIoKgpuHMaEGF/f3APp9NNPZ8KECSxevJjS0tI93+RffvllCgoKWLRoEdHR0fTo0cPvuPn6agvff/899913H5999hlt27blt7/9rd/rqI8+xtjY2D3PIyMjPTcNNXTNQw89lEWLFjFjxgz+9Kc/MXLkSP7whz/w6aef8v777zNlyhQefvhhPvjgA0/v41XrrhHUbBoyxrR4SUlJ5OXlcdFFF9XqJC4sLKRjx45ER0cza9YsfvjB9+zKw4cP5/nnnwdg2bJlLF26FIAdO3aQmJhIamoqGzdu5O23395zTnJyMkX1fGkcPnw4r7/+Ojt37qSkpIRp06ZxzDHHNKqcDV1z/fr1JCQkMHbsWK677joWL15McXExhYWFjBo1ismTJ7OkqdvssBqBMaaFGT16NL/61a9qjSA655xzGD16NIMHDyYnJ4fDDjvM5zUuu+wyLrzwQrKzs8nJyeGII44AYMCAAQwcOJB+/fpx8MEHc/TRR+85Z/z48Zx00klkZGQwa9asPftzc3P57W9/u+caF198MQMHDvTcDARwxx137OkQBmc6j/quOXPmTK6//noiIiKIjo7mkUceobi4mHPPPZeysjJUlQceeMDz+3qmqi1+GzRokPoza9asujuXLFEF1dde83t+qKq33GHAyt30li9fHrBrN9aOHTuCHULQeC17ff9+wEL18BnbupuGrLPYGGP8at2JoLqPwJqGjDGmQa07EViNwBhj/GrdiSA+HiIirEZgjDE+tO5EIOLUCqxGYIwxDWrdiQCcRGA1AmOMaVDrTwTVM5AaY1q0LVu2kJOTQ05ODp06daJLly57Xu/atcvTNS688EJWrVrl+T2feOIJrrnmmgMNudVo3TeUgTUNGRMi0tLS9tw1e+utt5KUlLRnps3qO373jHuPqP877NNPP908wbYy4VEjsKYhY0LWN998w5AhQ7j00kvJzc1lw4YNjB8/fs9U0rfddtueY4cNG8aSJUuoqKigTZs2TJw4kQEDBnDkkUeyadMmz+/53HPP0b9/f7KysrjxxhsBqKio4Lzzztuz/6GHHgLggQceoG/fvgwYMMDTzKMtUXjUCNatC3YUxoSWYMxD7cPKlSt59tln+de//gXA3XffTbt27aioqCA/P58zzzyTvn371jqnsLCQESNGcPfddzNhwgSeeuopJk6c6Pe91q5dy80338zChQtJTU3l+OOP580336RDhw5s3ryZL7/8EmDPdND33nsvP/zwAzExMQGZIro5tP4agXUWGxPyevbsyeGHH77n9Ysvvkhubi65ubmsWLGi3qmk4+PjOemkk4D9myJ6wYIFHHvssbRv357o6GjGjBnD3LlzyczMZNWqVVx99dXMnDmT1NRUAPr168fYsWN5/vnniY6Obnxhg6D11wiss9iY/ReseagbkJiYuOf56tWrefDBB/n0009p06YNY8eOrXcq6ZiYmD3PIyMjqaio8PRe2sAU0WlpaSxdupS3336bhx56iNdee43HHnuMmTNnMmfOHKZPn84dd9zBsmXLiIyM3M8SBpfVCIwxIWXHjh0kJyeTkpLChg0b6ix431hDhw5l1qxZbNmyhYqKCqZMmcKIESMoKChAVTnrrLOYNGkSixcvprKykrVr13Lsscfy17/+lYKCglrrHoeKgNUIRKQb8G+gE1AFPKaqD4rIrcAlQIF76I2qOiNQcZCcDCUlUFXl3GVsjAlpubm59O3bl6ysrDpTSR+IJ598stai9wsXLuS2224jLy8PVeWUU07h5JNPZvHixYwbNw5VRUS45557qKioYMyYMRQVFVFVVcUNN9xAcvUcZ6HEyxSlB7IBGUCu+zwZ+BroC9wKXLc/1zrgaahVVe+915mKuqjI7zVCkU3HHF5sGurw0xzTUAesRqCqG4AN7vMiEVkBdAnU+zWo5uI01c+NMcbs4betRETOEpFk9/nNIjJVRHL3501EpAcwEFjg7rpSRJaKyFMi0nY/Y94/tlylMcb4JOpjYWYAEVmqqtkiMgy4C7gPp11/iKc3EEkC5gB3qupUEUkHNgMK3A5kqOpF9Zw3HhgPkJ6ePqjmsnX1KS4uJqmeb/zt580j689/ZuGjj1J86KFeQg4pDZW7tbNyN73U1FQOOeSQehd9D7bKysqQG4nTVLyUXVX59ttvKSwsrLU/Pz9/kaoO9vsm/tqOgM/dx7uAMTX3eTg3GpgJTGjg5z2AZf6u06g+gvfec/oI5szxe41QZG3l4SWQ5f7uu++0oKBAq6qqAvYeB8r6CBpWVVWlBQUF+t1339X5GU3YR7BORB4FjgfuEZFYvDUpCfAksEJV76+xP0Od/gOAM4BlHmI4cLY4jTGedO3albVr11JQUOD/4GZWVlZGXFxcsMMICi9lj4uLo2vXrgf8Hl4SwdnAicB9qrpdRDKA6z2cdzRwHvCliFTfq34jMFpEcnCahtYAv9vvqPeHLVdpjCfR0dH07Nkz2GHUa/bs2QwcODDYYQRFc5TdSyLIAN5S1XIRyQOyce4P8ElV5wH1NTYG7p6B+liNwBhjfPJyh9VrQKWIZOI09fQEXghoVE2p5vBRY4wxdXhJBFWqWgH8Cpisqn/AqSWEBqsRGGOMT14SwW4RGQ2cD7zp7gudKfZiYpzNagTGGFMvL4ngQuBInPsAvheRnsBzgQ2ridkMpMYY0yC/iUBVlwPX4Yz+yQLWqurdAY+sKdlylcYY0yC/o4bckULP4gz1FKCbiFygqnMDG1oTsuUqjTGmQV6Gj/4NGKmqqwBE5FDgRWBQIANrUlYjMMaYBnnpI4iuTgIAqvo1odRZDLY4jTHG+OClRrBQRJ4E/uO+PhdYFLiQAiA5GX7+OdhRGGNMi+QlEVwGXAH8HqePYC7wj0AG1eSsRmCMMQ3ymwhUtRy4390AEJGXgHMCGFfTsuGjxhjToANdxPfIJo0i0Kyz2BhjGhQeq7knJUF5OezeHexIjDGmxWmwacjHcpRCqI0aqrlcZdvAroxpjDGhxlcfwd98/GxlUwcSUDVnILVEYIwxtTSYCFQ1vzkDCShbwN4YYxoUPn0EYInAGGPqER6JwJarNMaYBoVHIrAagTHGNMhvIhCR10TkZBEJ3aRhy1UaY0yDvHy4PwKMAVaLyN0icliAY2p61llsjDEN8rIwzXuqei6Qi7Mmwbsi8rGIXCgioXE/gdUIjDGmQZ6ae0QkDfgtcDHwOfAgTmJ4N2CRNaXEROfRagTGGFOHlxXKpgKH4UxDfYqqbnB/9JKILAxkcE0mIsJJBpYIjDGmDi/TUD+sqh/U9wNVHdzE8QSOLVdpjDH18pIIPhaRCcAwQIF5wCOqWhbQyJqazUBqjDH18pII/g0UAX93X4/GaSY6K1BBBYQtTmOMMfXykgh6q+qAGq9nicgX/k4SkW44SaQTUAU8pqoPikg74CWgB84opLNVddv+Br7fbHEaY4ypl5dRQ5+LyNDqFyIyBPjIw3kVwLWq2gcYClwhIn2BicD7qtoLeN99HXhWIzDGmHp5SQRDcPoJ1ojIGuATYISIfCkiSxs6SVU3qOpi93kRsALoApwGPOse9ixweiPi985qBMYYUy8vTUMnNvZNRKQHMBBYAKRXD0FV1Q0i0rGx1/fEOouNMaZeoqr+DxIZABzjvvxQVf32EdQ4NwmYA9ypqlNFZLuqtqnx822qWme1GBEZD4wHSE9PHzRlyhSf71NcXExS9R3E9ch8+GE6vfMO895802voIcFfuVsrK3d4CddyQ+PKnp+fv8jTMH9V9bkBVwPLgNvc7UvgKn/nuedGAzOBCTX2rQIy3OcZwCp/1xk0aJD6M2vWLN8H3HSTakSEalWV32uFEr/lbqWs3OElXMut2riyAwvVw2e1lz6CccAQVf0/Vf0/nI7fS/ydJCICPAmsUNX7a/zoDeAC9/kFwHQPMTReUhJUVUFpabO8nTHGhAovfQQCVNZ4Xenu8+do4DzgSxFZ4u67EbgbeFlExgE/0lz3I9ScgTQhoVne0hhjQoGXRPA0sEBEprmvT8f5pu+Tqs6j4YRxnLfwmlDNxWk6Nk//tDHGhAK/iUBV7xeR2ThTTAhwoap+HujAmpwtV2mMMfXymQjcVcmWqmoWsLh5QgoQW67SGGPq5bOzWFWrgC9E5KBmiidwbHEaY4ypl5c+ggzgKxH5FCip3qmqpwYsqkCw5SqNMaZeXhLBpIBH0RysRmCMMfXykghGqeoNNXeIyD04dwuHDqsRGGNMvbzcUHZCPftOaupAAs46i40xpl4N1ghE5DLgcuDgfWYZTQY+DnRgTS42FqKirGnIGGP24atp6AXgbeAuaq8ZUKSqWwMaVSCI2AykxhhTjwYTgaoWAoXAaBGJBNLd45NEJElVf2ymGJuOLU5jjDF1+O0sFpErgVuBjThLToKziH124MIKEFucxhhj6vAyauganHWLtwQ6mICzGoExxtThZdTQTzhNRKHPagTGGFOHlxrBd8BsEXkLKK/euc8aA6EhKQm2hH7FxhhjmpKXRPCju8W4W+hKTramIWOM2YeXaagnAYhIoqqW+Du+RbPho8YYU4ffPgIROVJElgMr3NcDROSfAY8sEKyz2Bhj6vDSWTwZ+AWwBUBVvwCGBzKogElOdtYsrqz0f6wxxoQJL4kAVf1pn12h+UlaPd9QSWi3cBljTFPyNHxURI4CVERiROQ63GaikGPLVRpjTB1eEsGlwBVAF2AtkOO+Dj02A6kxxtThZdTQZuDcZogl8GxxGmOMqcPLqKF7RSRFRKJF5H0R2SwiY5sjuCZni9MYY0wdXpqGRqrqDuCXOE1DhwLXBzSqQLEagTHG1OElEUS7j6OAF0NyLYJqViMwxpg6vEwx8V8RWQmUApeLSAegLLBhBYh1FhtjTB1+awSqOhE4EhisqruBEuC0QAcWEDZ81Bhj6vDSWXwWUKGqlSJyM/Ac0NnDeU+JyCYRWVZj360isk5ElrjbqEZFv78SE51HqxEYY8weXvoI/qyqRSIyDGeqiWeBRzyc9wxwYj37H1DVHHeb4T3UJhAVBXFxViMwxpgavCSC6ukkTgYeUdXpeJiOWlXnAi2vY9kWpzHGmFq8JIJ1IvIocDYwQ0RiPZ7XkCtFZKnbdNS2Edc5MDYDqTHG1CKq6vsAkQScJp4vVXW1iGQA/VX1f34vLtIDeFNVs9zX6cBmQIHbgQxVvaiBc8cD4wHS09MHTZkyxed7FRcXk1Q9KsiHwePGUZaRwbI77vB7bCjwWu7WxsodXsK13NC4sufn5y9S1cF+D1RVvxswALjS3QZ4Occ9rwewbH9/tu82aNAg9WfWrFl+j1FV1aOOUj3uOG/HhgDP5W5lrNzhJVzLrdq4sgML1cNnrJdRQ1cDzwMd3e05EbnqAJITbm2i2hnAsoaObQo7dsDKlfvstOUqjTGmFi83lI0Dhqi7TKWI3AN8Avzd10ki8iKQB7QXkbXALUCeiOTgNA2tAX53wJF7cO218MYbsHFjjZ1JSfDTvssrGGNM+PKSCITaC9FUuvt8UtXR9ex+0mNcTSIzEzZtcmoGKSnuTussNsaYWrwkgqeBBSIyzX19Os38gX6gevVyHr/5BnJz3Z02fNQYY2rxMsXE/cCFOPcEbAMuVNXJgQ6sKWRmOo/ffFNjZ1KSJQJjjKnBZ41ARCKApeoM/1zcPCE1nUMOcR5Xr66xMzkZdu+G8nKIjQ1KXMYY05L4rBGoahXwhYgc1EzxNKnEROjcuZ4aAVitwBhjXF76CDKAr0TkU5yZRwFQ1VMDFlUTyszcJxHUnIE0LS0oMRljTEviJRFMCngUAdSrF7z5Zo0dViMwxphaGkwEIpIJpKvqnH32DwfWBTqwppKZ6dxHUFTkVgZsuUpjjKnFVx/BZKC+T8ud7s9CQs0hpIAtV2mMMfvwlQh6qOrSfXeq6kKceYJCQp0hpNY0ZIwxtfhKBHE+fhbf1IEESp0hpLZcpTHG1OIrEXwmIpfsu1NExgGLAhdS00pKgowMqxEYY0xDfI0augaYJiLnsveDfzDO6mRnBDqwplRrCKl1FhtjTC0NJgJV3QgcJSL5QJa7+y1V/aBZImtCvXrBjOrVkRMSQMRqBMYY4/J7H4GqzgJmNUMsAZOZCT//7Hz2JyWJzUBqjDE1NGbt4ZBR7xBSqxEYYwwQJomg3iGklgiMMQYIk0RQ7xBSaxoyxhjA9xQTRThLStb5EaCqmlLPz1qk5GTo1MlqBMYYUx9fo4aSmzOQQKszhHT9+qDGY4wxLYXnpiER6SgiB1VvgQwqEHr12qdpyGoExhgDeEgEInKqiKwGvgfmAGuAtwMcV5PLzIQNG6CkBBs+aowxNXipEdwODAW+VtWewHHARwGNKgBqDSG1GoExxuzhJRHsVtUtQISIRLg3mOUEOK4mV2sIaVKSUzWoqgpqTMYY0xJ4WaFsu4gkAXOB50VkE1AR2LCaXnUiWL0ap0agCjt37p17yBhjwpSXGsFpOIvR/AF4B/gWOCWQQQVCcjKkp9eoEYA1DxljDN4SQUcgRlUrVPVZ4HEgJIeWZma6NYKMDGfH998HNR5jjGkJvCSCV4CajemV7r6Q06uXWyM4+mhnx5w5Po83xphw4CURRKnqruoX7vMYfyeJyFMisklEltXY105E3hWR1e5j2wML+8BkZjr3kZUkdIC+fS0RGGMM3hJBgYicWv1CRE4DNns47xngxH32TQTeV9VewPvu62ZTPYT022+BESNg3jyoCLl+b2OMaVJeEsGlwI0i8qOI/ATcAPzO30mqOhfYus/u04Bn3efPAqfvR6yNVmsI6YgRTmfx5583ZwjGGNPieFmY5ltgqDuEVFS1MbfkpqvqBve6G0SkYyOutd9qDSG9YITzYs4cOPzw5gzDGGNaFF+zj45V1edEZMI++wFQ1fsDGZiIjAfGA6SnpzN79myfxxcXF/s9BqBt26P48MPNDBnyNUd068bOqVNZNnhwE0QcHF7L3dpYucNLuJYbmqfsvmoEie5jUw4V3SgiGW5tIAPY1NCBqvoY8BjA4MGDNS8vz+eFZ8+ejb9jAPr0geLizuTldYZf/IKEV14h75hjIDJyf8rRYngtd2tj5Q4v4VpuaJ6y+5qG+lERiQR2qOoDTfR+bwAXAHe7j9Ob6Lqe9eoF773nvhgxAp54Ar78EnJCbtYMY4xpEj47i1W1EjjV1zENEZEXgU+A3iKyVkTG4SSAE9zZTE9wXzerzExYt86ZXYIRNfoJjDEmTHmZa+hjEXkYeAkoqd6pqot9naSqoxv40XHew2t6NYeQ9u/fDXr2dBLB1VcHMyxjjAkaL4ngKPfxthr7FDi26cMJvJpDSPv3x6kV/Pe/zkykEWGxhLMxxtTiZfhofnME0lxqDSEFJxE88wwsXw5ZWcEKyxhjgsbLCmWpInK/iCx0t7+JSGpzBBcIqanQoUON9Yutn8AYE+a8tIU8BRQBZ7vbDuDpQAYVaLXWL+7RA7p1s0RgjAlbXvoIDlHVX9d4PUlElgQqoOaQmQkffOC+EHFqBf/7n7NYjXvDnDHGhAsvNYJSERlW/UJEjgZKAxdS4GVmwtq17hBScBLBpk2walVQ4zLGmGDwkgguA/4hImtE5AfgYZyJ6ELWEUc4j48/7u6wfgJjTBjzmwhUdYmqDgCygf6qOlBVvwh8aIEzciSMGgU33QRr1uBUETIyLBEYY8KS3z6CBiadKwQWqWpI9hWIwCOPOGvTXHopvP22ICNGOInA+gmMMWHGS9PQYJymoC7uNh7IAx4XkT8GLrTAOugguOsumDkTXngBp3lo/Xp31RpjjAkfXhJBGpCrqteq6rU4iaEDMBz4bQBjC7jLL4ehQ53ZJbZmDXd2WvOQMSbMeEkEBwG7arzeDXRX1VKgPCBRNZPISKfDeMcOuPpffZw7zSwRGGPCjJdE8AIwX0RuEZFbgI+AF0UkEVge0OiaQVYWTJwIzz0v/Nx7uCUCY0zY8TJq6HbgEmA7Tifxpap6m6qWqOq5gQ6wOdx0Exx2GDzy1Qj48Ud3KJExxoQHr9NtxuMsUDMZ+EFEegYwpmYXG+s0Eb26zZ1QdcqU4AZkjDHNyMukc7cANwB/cndFA88FMqhgGDYMRlzWjxmMouLOu2HLlmCHZIwxzcJLjeAMnFXKSgBUdT1Nu45xi3HXXXBHyr1IcRHcfnuwwzHGmGbhJRHsUlXFWYwGt5O4VUpNhV/e0I+nuIiqf/zT7ikwxoQFL4ngZRF5FGgjIpcA7wFPBDas4LnySrg/9TZ2aTT86U/+TzDGmBDnZdTQfcCrwGtAb+D/VPWhQAcWLCkpMObaDO6pvA5eeQXmzw92SMYYE1BeOovvUdV3VfV6Vb1OVd8VkXuaI7hgueoqeCzlerbFpsN11znzDxljTCvlpWnohHr2ndTUgbQkbdrAJX9IYmL5JPjoI3j99WCHZIwxAdNgIhCRy0TkS6C3iCytsX0PLG2+EIPj6qvhleRxrE3uAzfcALt3BzskY4wJCF81gheAU4A33MfqbZCqjm2G2IKqbVu44uooLiu6x1ng+LHHgh2SMcYERIOJQFULVXWNqo5W1R9wlqdUIElEDmq2CIPommtgduIvWd5hBEya5MxOZ4wxrYyXzuJTRGQ18D0wB1gDvB3guFqEtDS46vfCBQX3QUEBjBsHpSG9XLMxxtThpbP4DmAo8LWq9gSOw5mBNCxMmAArEgfzQs698NprcMwxzsr3xhjTSnhJBLtVdQsQISIRqjoLyGnMm4rIGhH5UkSWiMjCxlwr0Nq3hyuugPOWXs/af0yHVatg8GD4+ONgh2aMMU3CSyLYLiJJwFzgeRF5EKhogvfOV9UcVR3cBNcKqOuug/h4GHTrKbxy7Xw0KQny8+Gpp4IdmjHGNJqXRHAasBP4A/AO8C3O6KGw0aEDfPgh9O4NZ0/qx7DoT9nSb7jTZ3DNNVDRFHnRGGOCQ7SBu2ZFJBNIV9WP9tk/HFinqgc8I5t7L8I2nFFIj6pqnbGZIjIeGA+Qnp4+aIqfNQKKi4tJSko60JA8UYW5c9vz2GOHsHF9NP/pdCWjf36U7dnZrPrjHynt0iWg71+f5ih3S2TlDi/hWm5oXNnz8/MXeWp1UdV6N+BNILue/YOB/zZ0npcN6Ow+dgS+AIb7On7QoEHqz6xZs/we01TKylT/+lfVlBTV8+Q/ujM2Vavi41X/9jfViopmi0O1ecvdkli5w0u4llu1cWUHFqqHz2RfTUM9VLXOHcSquhDosV9pqe411ruPm4BpwBGNuV5zi411+g2++QbiLh7LIeXL+TjheLj2Wjj6aFge8ks5G2PCiK9EEOfjZ/EH+oYikigiydXPgZHAsgO9XjB16ODccHzvfzpzfPF0rmz3ArtXfgMDB8Kdd9q0FMaYkOArEXxNsFi0AAATxElEQVTmrj9Qi4iMAxY14j3TgXki8gXwKfCWqr7TiOsF3dix8NHHwpvJozm4dDnf55wBN98Mubnw4ot1OpNtMlNjTEviKxFcA1woIrNF5G/uNge4GLj6QN9QVb9T1QHu1k9V7zzQa7UkubmwcCEcOqwjB386hcdHTUN3V8CYMZR268V7pz/M+b8uITMTEhKcAUcrVgQ7amOM8T3X0EZVPQqYhDOtxBpgkqoeqao/N094oaV9e5g50+kqGD/jdDps+opTmc7inztz/PSrmPx6d/4ScyuXnLGZF1+Evn3htNOcma6NMSZYvKxQNktV/+5uHzRHUKEsKgruuw9efhl+cVIER/7lVEpmfkThW/No98ujOXvFJB6a1o1tx/6al3/9EkvmFTNsmNPHPH06VFUFuwTGmHATFewAWquzznK2vY6GUdOd9qB//pPYV1/lrJ+ncmZ8PN/mjGLy6rMZc/rJdDo4kUsvhQsvdGoYxhgTaF7uLDZNqU8f+PvfnYnr5sxBLrqIzA3zeLjgHApjOvBo0Rg++OPbdO9SwfnnO0smW+eyMSaQLBEES2QkDB8ODz8M69bB7NlEXXQBx1e8w9uMYkNkV46YMoHLjvycQbnK22Ex8bcxJhgsEbQEkZEwYgQ88ghs2ABTp5Jy4lFcwcN8Ti5Tlvfn41F38MQ1y9Aqqx4YY5qWJYKWJjYWzjgDpk5Ffv4ZHnmEQ3JTuZ0/c/GD/dmY2ovdV1/rzIJXWRnsaI0xrYAlgpasXTu49FIiP/kIXbeed07/F4uLD0X//rDTrNSpE73vvRdmzIDy8mBHa4wJUZYIQoR0zuDEab+j6r8z6JlYwPjUl9g88AQ6zJ0LJ58MHTs6tzhPm2bLaRpj9osNHw0xv/wlvLsghVNPPZsuc87mwjEr+EP/78hc+hqR/50Ozz8PiYkwcqRzc8LQoc5tz/EHPD2UMaaVs0QQgvr2hU8/hTFj4NFn+vAofYiNPZnDc3YzpsscTih6jR4L3yFq2jTnhKgoyMmBIUOcxHDMMdC9e3ALYYxpMSwRhKh27eCdd+CVVz4hIuJI5s+H+fOjmTDjeMrKjgegf8eNjD54AXnx8+mzfT6pzzyD/OMfAOhB3SnOHc76zOGs6DCc1fTiuOOF3NxglsoYEwyWCEJchw7l5OXBr3/tvN61C5YuhU8+gQUL0nly/qnc+O2pAERHVHJS92X02zKXgT/OZfiPM+nNf+gNbKATCxjCskMPZ/Clg+l7/mBISwtauQ7EsmXOvXq33AKdOwc7GmNChyWCViYmBgYPdrarrnL2FRQ4TUnz50fy1VcD2Nx+ACu7XUVJV6W3fM3BP82hzRdzOWbOZ6R9PR0mABOgNKMnccMGI336OJ+sXbrs3dq3h4imGWvwzTfOJRvTjbF2LZx4onNv3syZzta7d5OEZ0yrZ4kgDHTo4AwsOvnkfX8iQG93G088ULyukBm3L2LV8wvpvWEhw974jE6vvEoEtW9k2y3RFCR0Z1vnLCr7ZJF8ZBadTuhPfHYviI72FNdPP8HEifDCC9C/P7z+Ohx88P6Xr7AQRo2CoiJ49lln9bijj3ZG1R4RUmvfhZ5ly2DCBJg0CY48MtjRmANlicDUktQllbP/dSylDxzLk0/C0Hthw0+76Rn3MwfHradnzDoOilxHF1lHp+JvOWj1Mg5b/QaRb1TBn2AX0WxM7kVUl3Ta9e5AbLeOTibq0AHS06FPH4o7H8o990Vy333Oe15+ubN+z+GHw0svwfHHe4931y6nWWzFCnj7befcI4+EX/wC8vPh1VfhpJMC87tqbcrK4Ior4KuvnMFnhxzi+/jFi+GEE2DrVvj8c1iw4MASuQk+SwSmXvHxcOWVzgeDajQREd2AbnWO270bVn9Vxk/vrqTok2VErFhG7A9fk7xyEztWLiEjqoCUim21zokknlPIIv+QAeScP4B2+QP441ldGX15W04cmcJf/xbBNdeAiO8YVeGSS+D99+GZZ/YmkF694OOPnQRw6qnw5JNw/vlN83tprX7+2bmhff58SEpyalKvvALHHlv/8fPnO01xbdo4SWPMGDjlFOf3npravLGbxrNEYHwS8f2BHB0Nh+XEcVhODpADOB/QixfDv6fCa6/Bt6t2057NZLffQPrmZZzUaQmjunxB6pppcMsTcAt0Bz4GKomgcEIqm/6vLR16tSWicydnqGuPHnu37t1BlVtugX//G267DS64oHZcnTrBnDnOh9sFFzhTOJ1+OuzY4TQlFRY6z4uK4KijnD6VcLVkiZMwt2xxalA5Oc7rkSPhwQedGlvNv4E5c5z7WTp1cpLwQQc5/84jR8LZZ8NbbzkjlkNJZSUUF4dvEguxfy4TCkRg0CBnu/NOWLEimqlTM/j44wx+MSaXc0af7/Qzqzq9u0uXwsaNsG0bEVu38e272/j60230+HYbOSXriJ/3ERGF22u9x7CIaPpUteWy1LZ0mtEWPmkLbds6X1FjYiAmhpSYGGYeHc3LBTEsnBjDfROTKSKZHaTU2m4ijWG/SOLP/yccdVRwfmfBMnUqnHeeMxx53jwYONDZ/8kncO65Tq1w6VJnNFZMDPzvf05C7dkT3nsPMjKc4/Pz4V//gosvhmuucSbVDQWVlU5z5KRJsHq106R46aVOf1qoJbPGCKOimmDp0wduuqmeH4hA167OVr0LOPwO2PAGnDQWir529qdFbic37Qf6p/xA79g1bPtqHf27bufEIduQwm2waRN8/bXzVX/XLmcrLydKlTHAGD8xls2MZdPMjqxK7kiHPh1od1hHZ/hsaqqztWmz93lSUv3VJBHn0zIuztliY/c+j4nx39bVjFThL3+Bm2927jN8/XXnG361lBRn35//DHfd5fTBjBsH48c7/57vvut0+9Q0bhysWgV//SscdpiTRFqqqionCd5yCyxfDllZTqf3iy86ia5LFyepXXxxrT/P/aLq/Dlu3rx3i4tzap9t2vg/v7gYvvgCSkoiDyyA/SAaAqueDB48WBcuXOjzmNmzZ5OXl9c8AbUgrbncP/3kdEBu2FB7W78ekpI2M3Nme5KT/VykstLpyCgvd/5n7dhReysshK1b2bVuE6vnbWLD0gJSd22iW+wm2rGVmPLiJimLRkVRGZ9MaUwKRZrM1opkNu5MoawyighRIkSJFCVCqogQRaKjiExOJKZtIvEdEklOTyS1SyKbd/5Mr6y+9SeaiIi9bXk1t6oqqKpix7ZKVq+s5OuVVXy5pJIfVxQzctBWxpy4lajCLU6v79atzu/JPYeqKrZsVtZ8X0WlRlCV0paBI9sTm5HmJMq0NKc6ER8PsbFURsbw59tjmPVxLPc9FMPRR1btTcw1EvSeNVlFKCoWvv9B+P57oXR3FFmD4zgsJ46oxL3lm79oEUOzs53Yam4lJXX3VW+qzhTvUVF7HjUikp9+qGLFop2UF5bSIWEnvbqWkpawEykrQ3ftorRwF+VFTqwx7CJaKqiMiKE8Io5yiWMXsZQSR5nGUSzJFEWmUhSRSlFEG4oiUymOSKV8l1C1s5TYqlLiKCOeUuIpJYZdRFFB26QK2retIC21grYplcRH7qJ0aynl23ZSUVSKlO4kuqKUBHbyxkUP8Jsnf3dAf3ciskhV/TZ8Wo3AtFjdujlbfWbPXkZycp7/i0RGOltcnPNtvkuXeg+LAfoBB5fC44/D6fc4CSeSCtrIDjonFpIeV0jHmO3EVZZQUACVNdaXjhBIa1tFXMQuYrScWC3b+1hZSmVRCSlFO0imiHaRO+iUVMTBbbcRE1GBIlQRQRWCIqgKVbsrkG1riN5YQsKKEhIpIZ4y2u7vL7GGFGCQu42u3rnI3dq02fuhnpTk/M7cxJKWFkFk5wg2rq0gM20bkZ9/C+9tge3b67xHJPCX6hdXeYsrGch2NwCer3vMUA/X2R0RS1lUEqWRSZRGJlKhkUhlBVJVSUSV8xhZVQFE0Cs6gTbd42nbOQFJiIeENIiLQ2JjSYiJISEmhsKyGBatiOHrbyOJZRcJkeUkRJQRH1FOvJQRRyntdxfRbdf3xO/aTnxZIfG7d9Qaaq0iVEbHUxUXj8bFUxkZQ+nuaIrLoyjeEMmWn6LYSBS7iaaURHbSAYmPJyEjgdRO8bTrmkD64CRvv8hGsERgTA3x8fD73zsdpIWFkJgYRWxsO0Ta1TquosLp3vj++73bunVQXgXlQFGNY0WcYZXds537Jbp3934vnqqTkJathm9WVfLZnBV0TutEZUkZlaXl6M4yqkrLqdxZTmlJFTtLlJJipXSn81hZqbRLi+CwfpH06RdBnyznMTEl0pmcsF07p28l0nfzQxt3q/NL2LbNqUWUlTnf9N1v/JvX7+LmP5az7udInO/VtTciIjm4p9L7UOWw3krv3krvXkoUFSz8uJzPPy7jy8/KKN5SRhxlpCWVsLW8DVt3J1HM3q08MhFNTKIyLpHIuOg9FaXYWEhIcIq475adDb/5jf8+gFRguLt5VlW1tzYSH49ERxO1T5NgApDm/tv+8INzs+ePPzrNU0fmOhMJ1/TT7Nn7E8EBsURgTD2ionzPsBEV5Xygd+8OgWyZE9l7M3deXiSZvTeTl5fl6VxV53M5NjZAwUVF7b1HZB/tgX+e67TKVVQ4LXSVlc7zigqnAtLQneTHHgfHuvGvWOHcJf722z+Tnd2Jwd321hS7dXM+NJvoBvemERHhdLB4ILJ3IFywWSIwppUSCWAS8CAiYu+38wMh4sy027cvDBy4kry8Tv5PMgekJeVSY4wxQWCJwBhjwlxQEoGInCgiq0TkGxGZGIwYjDHGOJo9EYhIJPAP4CSgLzBaRPo2dxzGGGMcwagRHAF8o6rfqeouYApwWhDiMMYYQxDuLBaRM4ETVfVi9/V5wBBVvXKf48YD4wHS09MHTZkyxed1i4uLSUoK/I0XLY2VO7xYucNPY8qen5/fYu8srm/ClTrZSFUfAx4DZ4oJf9MotOapFnyxcocXK3f4aY6yB6NpaC21J7bvCqwPQhzGGGMITtNQFPA1cBywDvgMGKOqX/k4pwD4wc+l2wObmyrOEGLlDi9W7vDTmLJ3V9W6t37vo9mbhlS1QkSuBGbizFH1lK8k4J7jtyAistBLW1hrY+UOL1bu8NMcZQ/KFBOqOgOYEYz3NsYYU5vdWWyMMWGuNSWCx4IdQJBYucOLlTv8BLzsIbFCmTHGmMBpTTUCY4wxByDkE0E4TWAnIk+JyCYRWVZjXzsReVdEVruPjVnNsEUSkW4iMktEVojIVyJytbu/VZddROJE5FMR+cIt9yR3f08RWeCW+yURiQl2rIEgIpEi8rmIvOm+bvXlFpE1IvKliCwRkYXuvoD/nYd0IgjDCeyeAU7cZ99E4H1V7QW8775ubSqAa1W1D87ytVe4/86tvezlwLGqOgDIAU4UkaHAPcADbrm3AeOCGGMgXQ2sqPE6XMqdr6o5NYaMBvzvPKQTAWE2gZ2qzgW27rP7NOBZ9/mzwOnNGlQzUNUNqrrYfV6E8+HQhVZednUUuy+j3U1xVnJ81d3f6soNICJdgZOBJ9zXQhiUuwEB/zsP9UTQBfipxuu17r5wkq6qG8D5wAQ6+jk+pIlID2AgsIAwKLvbPLIE2AS8C3wLbFfVCveQ1vo3Pxn4I1Dlvk4jPMqtwP9EZJE78SY0w995qK9Z7GkCO9M6iEgS8BpwjarucL4ktm6qWgnkiEgbYBrQp77DmjeqwBKRXwKbVHWRiORV767n0FZVbtfRqrpeRDoC74rIyuZ401CvEdgEdrBRRDIA3MdNQY4nIEQkGicJPK+qU93dYVF2AFXdDszG6SNp487ZBa3zb/5o4FQRWYPT3HssTg2htZcbVV3vPm7CSfxH0Ax/56GeCD4DermjCWKA3wBvBDmm5vYGcIH7/AJgehBjCQi3ffhJYIWq3l/jR6267CLSwa0JICLxwPE4/SOzgDPdw1pduVX1T6raVVV74Pyf/kBVz6WVl1tEEkUkufo5MBJYRjP8nYf8DWUiMgrn20L1BHZ3BjmkgBGRF4E8nNkINwK3AK8DLwMHAT8CZ6nqvh3KIU1EhgEfAl+yt834Rpx+glZbdhHJxukcjMT50vayqt4mIgfjfFNuB3wOjFXV8uBFGjhu09B1qvrL1l5ut3zT3JdRwAuqeqeIpBHgv/OQTwTGGGMaJ9SbhowxxjSSJQJjjAlzlgiMMSbMWSIwxpgwZ4nAGGPCnCUCYwARqXRnfKzemmxiLxHpUXPGWGNamlCfYsKYplKqqjnBDsKYYLAagTE+uPPD3+OuC/CpiGS6+7uLyPsistR9PMjdny4i09w1BL4QkaPcS0WKyOPuugL/c+8UNqZFsERgjCN+n6ahc2r8bIeqHgE8jHMXO+7zf6tqNvA88JC7/yFgjruGQC7wlbu/F/APVe0HbAd+HeDyGOOZ3VlsDCAixaqaVM/+NTiLw3znTnz3s6qmichmIENVd7v7N6hqexEpALrWnPrAnTr7XXdhEUTkBiBaVe8IfMmM8c9qBMb4pw08b+iY+tScE6cS658zLYglAmP8O6fG4yfu849xZsYEOBeY5z5/H7gM9iwqk9JcQRpzoOxbiTGOeHclsGrvqGr1ENJYEVmA88VptLvv98BTInI9UABc6O6/GnhMRMbhfPO/DNgQ8OiNaQTrIzDGB7ePYLCqbg52LMYEijUNGWNMmLMagTHGhDmrERhjTJizRGCMMWHOEoExxoQ5SwTGGBPmLBEYY0yYs0RgjDFh7v8BvdiAWSv29hsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Get model history\n",
    "history=model.history\n",
    "\n",
    "#Plot train vs test loss\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('Epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
    "\n",
    "#List of epoch numbers\n",
    "x = list(range(1,epochs+1))\n",
    "\n",
    "#Display the loss\n",
    "val_loss = history.history['val_loss'] #Validation Loss\n",
    "loss = history.history['loss'] #Training Loss\n",
    "plt_loss(x, val_loss, loss, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inception V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Get the bottleneck features using a pre-trained InceptionV3 on ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 5 classes.\n",
      "Found 2500 images belonging to 5 classes.\n",
      "Got the bottleneck features in time:  0:35:39.074486\n"
     ]
    }
   ],
   "source": [
    "get_bottleneck_features(\"InceptionV3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train a model with the bottleneck features obtained using InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 73728)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               18874624  \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 18,943,749\n",
      "Trainable params: 18,942,725\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 199s 20ms/step - loss: 26.1691 - acc: 0.9634 - val_loss: 7.8130 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.99360, saving model to weights/inceptionV3_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 166s 17ms/step - loss: 6.9225 - acc: 0.9672 - val_loss: 5.6859 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.99360\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 173s 17ms/step - loss: 5.9783 - acc: 0.9638 - val_loss: 5.6588 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.99360\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 167s 17ms/step - loss: 5.3521 - acc: 0.9640 - val_loss: 5.3013 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.99360\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 168s 17ms/step - loss: 4.8489 - acc: 0.9648 - val_loss: 4.1162 - val_acc: 0.9772\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.99360\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 170s 17ms/step - loss: 4.4735 - acc: 0.9644 - val_loss: 4.8497 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.99360 to 0.99400, saving model to weights/inceptionV3_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 167s 17ms/step - loss: 4.0930 - acc: 0.9678 - val_loss: 3.3294 - val_acc: 0.9912\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.99400\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 168s 17ms/step - loss: 3.8400 - acc: 0.9650 - val_loss: 3.6897 - val_acc: 0.9592\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.99400\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 177s 18ms/step - loss: 3.5848 - acc: 0.9670 - val_loss: 3.6890 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.99400\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 170s 17ms/step - loss: 3.3144 - acc: 0.9690 - val_loss: 2.9844 - val_acc: 0.9864\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.99400\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 172s 17ms/step - loss: 3.1893 - acc: 0.9667 - val_loss: 3.1574 - val_acc: 0.9732\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.99400\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 195s 19ms/step - loss: 3.0086 - acc: 0.9665 - val_loss: 3.1155 - val_acc: 0.9588\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.99400\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 198s 20ms/step - loss: 2.8766 - acc: 0.9689 - val_loss: 2.8553 - val_acc: 0.9020\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.99400\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 193s 19ms/step - loss: 2.7918 - acc: 0.9671 - val_loss: 3.1072 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.99400\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 198s 20ms/step - loss: 2.6543 - acc: 0.9697 - val_loss: 2.5758 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.99400\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 200s 20ms/step - loss: 2.5821 - acc: 0.9690 - val_loss: 2.0816 - val_acc: 0.9996\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.99400 to 0.99960, saving model to weights/inceptionV3_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 197s 20ms/step - loss: 2.4871 - acc: 0.9713 - val_loss: 2.3667 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.99960\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 196s 20ms/step - loss: 2.3715 - acc: 0.9719 - val_loss: 2.1157 - val_acc: 0.9996\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.99960\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 195s 19ms/step - loss: 2.2452 - acc: 0.9747 - val_loss: 2.4789 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.99960\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 195s 19ms/step - loss: 2.1815 - acc: 0.9740 - val_loss: 2.2312 - val_acc: 0.9912\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.99960\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 189s 19ms/step - loss: 2.1451 - acc: 0.9741 - val_loss: 1.9349 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.99960\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 183s 18ms/step - loss: 2.1292 - acc: 0.9712 - val_loss: 1.8499 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.99960\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 184s 18ms/step - loss: 2.0512 - acc: 0.9731 - val_loss: 2.1410 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.99960\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 193s 19ms/step - loss: 2.0095 - acc: 0.9738 - val_loss: 1.8264 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.99960\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 191s 19ms/step - loss: 1.9455 - acc: 0.9755 - val_loss: 1.8804 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.99960\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 191s 19ms/step - loss: 1.9547 - acc: 0.9735 - val_loss: 1.9608 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.99960\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 189s 19ms/step - loss: 1.8977 - acc: 0.9753 - val_loss: 1.7542 - val_acc: 0.9728\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.99960\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 188s 19ms/step - loss: 1.9180 - acc: 0.9731 - val_loss: 1.9738 - val_acc: 0.9536\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.99960\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 185s 18ms/step - loss: 1.7992 - acc: 0.9769 - val_loss: 1.7980 - val_acc: 0.9864\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.99960\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 182s 18ms/step - loss: 1.8261 - acc: 0.9742 - val_loss: 1.8660 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.99960\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 185s 19ms/step - loss: 1.8353 - acc: 0.9727 - val_loss: 1.8590 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.99960\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 184s 18ms/step - loss: 1.7753 - acc: 0.9773 - val_loss: 1.6602 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.99960\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 184s 18ms/step - loss: 1.7605 - acc: 0.9753 - val_loss: 1.6295 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.99960\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 189s 19ms/step - loss: 1.7642 - acc: 0.9739 - val_loss: 1.6467 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.99960 to 1.00000, saving model to weights/inceptionV3_bottleneck_feats_multi_weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 191s 19ms/step - loss: 1.7143 - acc: 0.9761 - val_loss: 1.7377 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 1.00000\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 192s 19ms/step - loss: 1.7278 - acc: 0.9756 - val_loss: 1.5218 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 1.00000\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 191s 19ms/step - loss: 1.7022 - acc: 0.9760 - val_loss: 1.6201 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 1.00000\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 200s 20ms/step - loss: 1.7148 - acc: 0.9750 - val_loss: 1.6147 - val_acc: 0.9856\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 1.00000\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 195s 20ms/step - loss: 1.6631 - acc: 0.9758 - val_loss: 1.5949 - val_acc: 0.9916\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 1.00000\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 182s 18ms/step - loss: 1.6843 - acc: 0.9758 - val_loss: 1.5699 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 1.00000\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 186s 19ms/step - loss: 1.6753 - acc: 0.9773 - val_loss: 1.4504 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 1.00000\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 175s 18ms/step - loss: 1.6861 - acc: 0.9750 - val_loss: 1.9034 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 1.00000\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 181s 18ms/step - loss: 1.6803 - acc: 0.9726 - val_loss: 1.6872 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 1.00000\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 186s 19ms/step - loss: 1.6600 - acc: 0.9763 - val_loss: 1.4130 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 1.00000\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 186s 19ms/step - loss: 1.6366 - acc: 0.9783 - val_loss: 3.2898 - val_acc: 0.7148\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 1.00000\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 185s 18ms/step - loss: 1.6386 - acc: 0.9786 - val_loss: 1.4542 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 1.00000\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 184s 18ms/step - loss: 1.6538 - acc: 0.9754 - val_loss: 1.4735 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 1.00000\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 182s 18ms/step - loss: 1.6694 - acc: 0.9763 - val_loss: 1.5508 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 1.00000\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 181s 18ms/step - loss: 1.6678 - acc: 0.9751 - val_loss: 1.6898 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 1.00000\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 184s 18ms/step - loss: 1.6733 - acc: 0.9750 - val_loss: 2.0721 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 1.00000\n",
      "\n",
      "The top layer trained in time:  2:35:15.306058\n"
     ]
    }
   ],
   "source": [
    "#After we get the bottleneck features, we will build the top fully connected layers on top of the bottlneck features. Let's build the top layers.\n",
    "def train_model_inceptionV3():\n",
    "    global_start=dt.now()\n",
    "\n",
    "    train_data = np.load('cnn_codes/InceptionV3_bottleneck_features_train.npy')\n",
    "    validation_data = np.load('cnn_codes/InceptionV3_bottleneck_features_validation.npy')\n",
    "    \n",
    "    #train_labels = np.array([0] * (nb_train_samples // 3) + [1] * (nb_train_samples // 3) + [2] * (nb_train_samples // 3)) #Equivalent to: np.array([0]*1200 + [1]*1200 + [2]*1200)\n",
    "    #validation_labels = np.array([0] * (nb_validation_samples // 3) + [1] * (nb_validation_samples // 3) + [2] * (nb_validation_samples // 3))\n",
    "    train_labels=generator_tr.classes  \n",
    "    validation_labels=generator_ts.classes\n",
    "    \n",
    "    train_labels = to_categorical(train_labels, num_classes=num_classes)  \n",
    "    validation_labels = to_categorical(validation_labels, num_classes=num_classes)  \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:])) #Ignore the first index. It contains ID\n",
    "\n",
    "    model.add(Dense(256, activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001))) #Best weight initializer for relu is he_normal\n",
    "    model.add(BatchNormalization()) #Add a BatchNormalization layer to control internel covariance shift\n",
    "    model.add(Dropout(rate=0.5)) #Using droput for regularization\n",
    "\n",
    "    model.add(Dense(256, activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001)))\n",
    "    model.add(BatchNormalization()) #Add a BatchNormalization layer to control internel covariance shift\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax',kernel_initializer='glorot_uniform')) #Because we have 3 classes. Remember, softmax is to multi-class, what sigmoid (log reg) is to binary\n",
    "\n",
    "    optim=RMSprop(lr=0.0001, epsilon=1e-8, decay=1e-6)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    #Save the weights for the best epoch accuracy\n",
    "    checkpointer = ModelCheckpoint(filepath=\"weights/inceptionV3_bottleneck_feats_multi_weights.hdf5\", monitor = 'val_acc',verbose=1, save_best_only=True)\n",
    "                                   \n",
    "    model.fit(x=train_data,\n",
    "              y=train_labels,\n",
    "              epochs=epochs,\n",
    "              validation_data=(validation_data, validation_labels),\n",
    "              callbacks=[checkpointer])    \n",
    "    \n",
    "    #Refit our model with the best weights saved before\n",
    "    model.load_weights('weights/inceptionV3_bottleneck_feats_multi_weights.hdf5')\n",
    "    model.save('models/inception_v3__botlnck_trained.h5')\n",
    "    print(\"\\nThe top layer trained in time: \",dt.now()-global_start)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model=train_model_inceptionV3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Get model performance (InceptionV3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6466785596847535\n",
      "Validation Accuracy on Unseen Data): 1.0\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('cnn_codes/InceptionV3_bottleneck_features_train.npy')\n",
    "validation_data = np.load('cnn_codes/InceptionV3_bottleneck_features_validation.npy')\n",
    "\n",
    "train_labels = to_categorical(generator_tr.classes, num_classes=num_classes)  \n",
    "validation_labels = to_categorical(generator_ts.classes, num_classes=num_classes)\n",
    "\n",
    "#Plot the train and test loss vs number of epochs\n",
    "score = model.evaluate(validation_data, validation_labels, verbose=0) \n",
    "print('Validation Loss:', score[0]) \n",
    "print('Validation Accuracy on Unseen Data):', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Visualize the train and validation loss for InceptionV3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Get model history\n",
    "history=model.history\n",
    "\n",
    "#Plot train vs test loss\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('Epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
    "\n",
    "#List of epoch numbers\n",
    "x = list(range(1,epochs+1))\n",
    "\n",
    "#Display the loss\n",
    "val_loss = history.history['val_loss'] #Validation Loss\n",
    "loss = history.history['loss'] #Training Loss\n",
    "plt_loss(x, val_loss, loss, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Get the bottleneck features using a pre-trained ResNet50 on Imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 5 classes.\n",
      "Found 2500 images belonging to 5 classes.\n",
      "Got the bottleneck features in time:  0:51:25.690934\n"
     ]
    }
   ],
   "source": [
    "get_bottleneck_features(\"ResNet50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train a model with the bottleneck features obtained using ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 131072)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33554688  \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 33,623,813\n",
      "Trainable params: 33,622,789\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 306s 31ms/step - loss: 17.1175 - acc: 0.4561 - val_loss: 10.4775 - val_acc: 0.3852\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.38520, saving model to weights/resnet50_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 308s 31ms/step - loss: 8.7477 - acc: 0.5403 - val_loss: 17.2609 - val_acc: 0.2000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.38520\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 309s 31ms/step - loss: 7.9199 - acc: 0.5642 - val_loss: 11.8700 - val_acc: 0.2044\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.38520\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 312s 31ms/step - loss: 7.3530 - acc: 0.5819 - val_loss: 7.5692 - val_acc: 0.4328\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.38520 to 0.43280, saving model to weights/resnet50_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 328s 33ms/step - loss: 6.9790 - acc: 0.5869 - val_loss: 11.7249 - val_acc: 0.3104\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.43280\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 345s 34ms/step - loss: 6.6232 - acc: 0.5979 - val_loss: 7.0252 - val_acc: 0.4276\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.43280\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 347s 35ms/step - loss: 6.2378 - acc: 0.6132 - val_loss: 12.4222 - val_acc: 0.3004\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.43280\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 347s 35ms/step - loss: 5.8933 - acc: 0.6251 - val_loss: 7.8696 - val_acc: 0.3940\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.43280\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 347s 35ms/step - loss: 5.5771 - acc: 0.6462 - val_loss: 7.4756 - val_acc: 0.2996\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.43280\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 351s 35ms/step - loss: 5.3954 - acc: 0.6302 - val_loss: 8.0954 - val_acc: 0.2424\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.43280\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 356s 36ms/step - loss: 5.1570 - acc: 0.6429 - val_loss: 5.1130 - val_acc: 0.6468\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.43280 to 0.64680, saving model to weights/resnet50_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 337s 34ms/step - loss: 4.9474 - acc: 0.6395 - val_loss: 5.2353 - val_acc: 0.5548\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.64680\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 332s 33ms/step - loss: 4.7766 - acc: 0.6398 - val_loss: 6.1704 - val_acc: 0.4960\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.64680\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 331s 33ms/step - loss: 4.5762 - acc: 0.6511 - val_loss: 9.8394 - val_acc: 0.2032\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.64680\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 337s 34ms/step - loss: 4.4101 - acc: 0.6625 - val_loss: 6.6679 - val_acc: 0.4024\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.64680\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 333s 33ms/step - loss: 4.2457 - acc: 0.6582 - val_loss: 5.1270 - val_acc: 0.3996\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.64680\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 331s 33ms/step - loss: 4.1229 - acc: 0.6697 - val_loss: 5.2406 - val_acc: 0.5252\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.64680\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 330s 33ms/step - loss: 3.9786 - acc: 0.6741 - val_loss: 4.4979 - val_acc: 0.4812\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.64680\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 329s 33ms/step - loss: 3.8567 - acc: 0.6789 - val_loss: 7.7696 - val_acc: 0.2164\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.64680\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 331s 33ms/step - loss: 3.7562 - acc: 0.6775 - val_loss: 5.2637 - val_acc: 0.5544\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.64680\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 333s 33ms/step - loss: 3.6459 - acc: 0.6904 - val_loss: 3.5283 - val_acc: 0.6860\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.64680 to 0.68600, saving model to weights/resnet50_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 340s 34ms/step - loss: 3.5523 - acc: 0.6934 - val_loss: 4.9859 - val_acc: 0.4276\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.68600\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 335s 34ms/step - loss: 3.4793 - acc: 0.6948 - val_loss: 4.1268 - val_acc: 0.4472\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.68600\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 329s 33ms/step - loss: 3.4338 - acc: 0.6905 - val_loss: 4.8762 - val_acc: 0.5132\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.68600\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 326s 33ms/step - loss: 3.3179 - acc: 0.7068 - val_loss: 3.3150 - val_acc: 0.6572\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.68600\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 325s 32ms/step - loss: 3.2805 - acc: 0.7050 - val_loss: 5.0416 - val_acc: 0.4900\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.68600\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 318s 32ms/step - loss: 3.2295 - acc: 0.7050 - val_loss: 2.8784 - val_acc: 0.8508\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.68600 to 0.85080, saving model to weights/resnet50_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 327s 33ms/step - loss: 3.1622 - acc: 0.7133 - val_loss: 3.8692 - val_acc: 0.4616\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.85080\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 327s 33ms/step - loss: 3.1225 - acc: 0.7117 - val_loss: 4.1060 - val_acc: 0.4352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00029: val_acc did not improve from 0.85080\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 324s 32ms/step - loss: 3.1222 - acc: 0.7002 - val_loss: 5.7723 - val_acc: 0.3244\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.85080\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 327s 33ms/step - loss: 3.0628 - acc: 0.7106 - val_loss: 3.3840 - val_acc: 0.5252\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.85080\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 333s 33ms/step - loss: 3.0388 - acc: 0.7076 - val_loss: 4.3927 - val_acc: 0.4840\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.85080\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 329s 33ms/step - loss: 3.0135 - acc: 0.7097 - val_loss: 3.1652 - val_acc: 0.6764\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.85080\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 327s 33ms/step - loss: 2.9674 - acc: 0.7187 - val_loss: 2.7190 - val_acc: 0.8320\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.85080\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 327s 33ms/step - loss: 2.9415 - acc: 0.7223 - val_loss: 3.5273 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.85080\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 323s 32ms/step - loss: 2.9180 - acc: 0.7206 - val_loss: 4.1441 - val_acc: 0.5628\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.85080\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 321s 32ms/step - loss: 2.8982 - acc: 0.7232 - val_loss: 3.4871 - val_acc: 0.5888\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.85080\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 328s 33ms/step - loss: 2.8875 - acc: 0.7256 - val_loss: 4.0932 - val_acc: 0.4260\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.85080\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 331s 33ms/step - loss: 2.8458 - acc: 0.7290 - val_loss: 3.3910 - val_acc: 0.5824\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.85080\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 332s 33ms/step - loss: 2.8585 - acc: 0.7213 - val_loss: 5.5124 - val_acc: 0.2952\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.85080\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 342s 34ms/step - loss: 2.8293 - acc: 0.7168 - val_loss: 3.9302 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.85080\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 346s 35ms/step - loss: 2.8252 - acc: 0.7228 - val_loss: 2.9995 - val_acc: 0.6040\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.85080\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 349s 35ms/step - loss: 2.7950 - acc: 0.7341 - val_loss: 2.8815 - val_acc: 0.6536\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.85080\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 357s 36ms/step - loss: 2.7942 - acc: 0.7295 - val_loss: 4.8819 - val_acc: 0.2524\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.85080\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 351s 35ms/step - loss: 2.7823 - acc: 0.7366 - val_loss: 2.5981 - val_acc: 0.8068\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.85080\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 349s 35ms/step - loss: 2.7723 - acc: 0.7377 - val_loss: 3.2296 - val_acc: 0.5312\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.85080\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 351s 35ms/step - loss: 2.7565 - acc: 0.7370 - val_loss: 7.5624 - val_acc: 0.2092\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.85080\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 348s 35ms/step - loss: 2.7580 - acc: 0.7285 - val_loss: 2.7982 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.85080\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 350s 35ms/step - loss: 2.7266 - acc: 0.7364 - val_loss: 5.4068 - val_acc: 0.2316\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.85080\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 355s 36ms/step - loss: 2.7282 - acc: 0.7430 - val_loss: 3.1776 - val_acc: 0.5924\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.85080\n",
      "\n",
      "The top layer trained in time:  4:40:00.617158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/resnet_using_bottleneck_best.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After we get the bottleneck features, we will build the top fully connected layers on top of the bottlneck features. Let's build the top layers.\n",
    "def train_model_resnet50():\n",
    "    global_start=dt.now()\n",
    "\n",
    "    train_data = np.load('cnn_codes/ResNet50_bottleneck_features_train.npy')\n",
    "    validation_data = np.load('cnn_codes/ResNet50_bottleneck_features_validation.npy')\n",
    "    \n",
    "    #train_labels = np.array([0] * (nb_train_samples // 3) + [1] * (nb_train_samples // 3) + [2] * (nb_train_samples // 3)) #Equivalent to: np.array([0]*1200 + [1]*1200 + [2]*1200)\n",
    "    #validation_labels = np.array([0] * (nb_validation_samples // 3) + [1] * (nb_validation_samples // 3) + [2] * (nb_validation_samples // 3))\n",
    "    train_labels=generator_tr.classes  \n",
    "    validation_labels=generator_ts.classes\n",
    "    \n",
    "    train_labels = to_categorical(train_labels, num_classes=num_classes)  \n",
    "    validation_labels = to_categorical(validation_labels, num_classes=num_classes)  \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:])) #Ignore the first index. It contains ID\n",
    "\n",
    "    model.add(Dense(256, activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001))) #Best weight initializer for relu is he_normal\n",
    "    model.add(BatchNormalization()) #Add a BatchNormalization layer to control internel covariance shift\n",
    "    model.add(Dropout(rate=0.5)) #Using droput for regularization\n",
    "\n",
    "    model.add(Dense(256, activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001)))\n",
    "    model.add(BatchNormalization()) #Add a BatchNormalization layer to control internel covariance shift\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax',kernel_initializer='glorot_uniform')) #Because we have 3 classes. Remember, softmax is to multi-class, what sigmoid (log reg) is to binary\n",
    "\n",
    "    optim=RMSprop(lr=0.0001, epsilon=1e-8, decay=1e-6)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    #Save the weights for the best epoch accuracy\n",
    "    checkpointer = ModelCheckpoint(filepath=\"weights/resnet50_bottleneck_feats_multi_weights.hdf5\", monitor = 'val_acc',verbose=1, save_best_only=True)\n",
    "                                   \n",
    "    model.fit(x=train_data,\n",
    "              y=train_labels,\n",
    "              epochs=50,\n",
    "              validation_data=(validation_data, validation_labels),\n",
    "              callbacks=[checkpointer])    \n",
    "    \n",
    "    #Refit our model with the best weights saved before\n",
    "    model.load_weights('weights/resnet50_bottleneck_feats_multi_weights.hdf5')\n",
    "    model.save('models/resnet50__botlnck_trained.h5')\n",
    "    print(\"\\nThe top layer trained in time: \",dt.now()-global_start)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model=train_model_resnet50()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Get model performance (ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score (Validation Loss): 6.141017456054687\n",
      "Test accuracy (Accuracy on Unseen Data): 0.965\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('cnn_codes/ResNet50_bottleneck_features_train.npy')\n",
    "validation_data = np.load('cnn_codes/ResNet50_bottleneck_features_validation.npy')\n",
    "\n",
    "train_labels = to_categorical(generator_tr.classes, num_classes=num_classes)  \n",
    "validation_labels = to_categorical(generator_ts.classes, num_classes=num_classes)\n",
    "\n",
    "#Plot the train and test loss vs number of epochs\n",
    "score = model.evaluate(validation_data, validation_labels, verbose=0) \n",
    "print('Validation Loss:', score[0]) \n",
    "print('Validation Accuracy on Unseen Data):', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Visualize the train and validation loss for ResNet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Get model history\n",
    "history=model.history\n",
    "\n",
    "#Plot train vs test loss\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('Epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
    "\n",
    "#List of epoch numbers\n",
    "x = list(range(1,epochs+1))\n",
    "\n",
    "#Display the loss\n",
    "val_loss = history.history['val_loss'] #Validation Loss\n",
    "loss = history.history['loss'] #Training Loss\n",
    "plt_dynamic_loss(x, val_loss, loss, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DenseNet201"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Get the bottleneck features using a pre-trained DenseNet201 on Imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 5 classes.\n",
      "Found 2500 images belonging to 5 classes.\n",
      "Got the bottleneck features in time:  1:17:11.040657\n"
     ]
    }
   ],
   "source": [
    "get_bottleneck_features(\"DenseNet201\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train a model with the bottleneck features obtained using DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 122880)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               31457536  \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 31,526,661\n",
      "Trainable params: 31,525,637\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 301s 30ms/step - loss: 21.0599 - acc: 0.9732 - val_loss: 7.2477 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.99840, saving model to weights/densenet201_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 289s 29ms/step - loss: 6.8704 - acc: 0.9802 - val_loss: 5.8838 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.99840 to 0.99880, saving model to weights/densenet201_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 297s 30ms/step - loss: 6.0059 - acc: 0.9772 - val_loss: 5.4737 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.99880\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 302s 30ms/step - loss: 5.3292 - acc: 0.9784 - val_loss: 5.5574 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.99880\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 316s 32ms/step - loss: 4.8908 - acc: 0.9765 - val_loss: 4.5947 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.99880\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 348s 35ms/step - loss: 4.4884 - acc: 0.9774 - val_loss: 4.4003 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.99880\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 336s 34ms/step - loss: 4.1750 - acc: 0.9777 - val_loss: 4.1553 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.99880\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 348s 35ms/step - loss: 3.8920 - acc: 0.9778 - val_loss: 5.0782 - val_acc: 0.7340\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.99880\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 339s 34ms/step - loss: 3.6752 - acc: 0.9785 - val_loss: 4.2954 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.99880\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 329s 33ms/step - loss: 3.5303 - acc: 0.9776 - val_loss: 3.4418 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.99880\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 328s 33ms/step - loss: 3.3729 - acc: 0.9791 - val_loss: 2.8373 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.99880\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 322s 32ms/step - loss: 3.2562 - acc: 0.9788 - val_loss: 2.9545 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.99880\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 324s 32ms/step - loss: 3.1292 - acc: 0.9787 - val_loss: 3.1808 - val_acc: 0.9796\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.99880\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 340s 34ms/step - loss: 3.0057 - acc: 0.9805 - val_loss: 2.8859 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.99880\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 343s 34ms/step - loss: 2.9784 - acc: 0.9793 - val_loss: 3.1239 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.99880\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 341s 34ms/step - loss: 2.9018 - acc: 0.9804 - val_loss: 2.3005 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.99880\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 329s 33ms/step - loss: 2.8209 - acc: 0.9796 - val_loss: 2.9992 - val_acc: 0.9992\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.99880 to 0.99920, saving model to weights/densenet201_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 324s 32ms/step - loss: 2.7311 - acc: 0.9797 - val_loss: 2.9758 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.99920\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 304s 30ms/step - loss: 2.7327 - acc: 0.9796 - val_loss: 2.7419 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.99920\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 285s 29ms/step - loss: 2.6773 - acc: 0.9799 - val_loss: 2.5140 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.99920\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 281s 28ms/step - loss: 2.6307 - acc: 0.9806 - val_loss: 2.5690 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.99920\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 288s 29ms/step - loss: 2.6145 - acc: 0.9795 - val_loss: 2.6987 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.99920\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 293s 29ms/step - loss: 2.5638 - acc: 0.9807 - val_loss: 3.2132 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.99920\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 318s 32ms/step - loss: 2.5092 - acc: 0.9819 - val_loss: 2.4939 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.99920\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 324s 32ms/step - loss: 2.4910 - acc: 0.9798 - val_loss: 2.3364 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.99920 to 1.00000, saving model to weights/densenet201_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 330s 33ms/step - loss: 2.4123 - acc: 0.9833 - val_loss: 3.0807 - val_acc: 0.8772\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 1.00000\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 366s 37ms/step - loss: 2.3791 - acc: 0.9845 - val_loss: 2.2159 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 1.00000\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 356s 36ms/step - loss: 2.3746 - acc: 0.9826 - val_loss: 2.4651 - val_acc: 0.9768\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 1.00000\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 330s 33ms/step - loss: 2.3855 - acc: 0.9816 - val_loss: 2.5126 - val_acc: 0.9728\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 1.00000\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 330s 33ms/step - loss: 2.3905 - acc: 0.9808 - val_loss: 2.8720 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 1.00000\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 335s 33ms/step - loss: 2.3659 - acc: 0.9812 - val_loss: 2.5566 - val_acc: 0.9864\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 1.00000\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 332s 33ms/step - loss: 2.3938 - acc: 0.9808 - val_loss: 3.4078 - val_acc: 0.7732\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 1.00000\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 332s 33ms/step - loss: 2.3148 - acc: 0.9825 - val_loss: 2.3835 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 1.00000\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 347s 35ms/step - loss: 2.3174 - acc: 0.9837 - val_loss: 2.3965 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 1.00000\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 377s 38ms/step - loss: 2.3141 - acc: 0.9814 - val_loss: 2.5158 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 1.00000\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 395s 40ms/step - loss: 2.2704 - acc: 0.9831 - val_loss: 2.4520 - val_acc: 0.9708\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 1.00000\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 389s 39ms/step - loss: 2.3132 - acc: 0.9806 - val_loss: 2.4921 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 1.00000\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 394s 39ms/step - loss: 2.2850 - acc: 0.9819 - val_loss: 2.1729 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 1.00000\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 369s 37ms/step - loss: 2.3149 - acc: 0.9807 - val_loss: 2.1360 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 1.00000\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 390s 39ms/step - loss: 2.2668 - acc: 0.9815 - val_loss: 2.4572 - val_acc: 0.9604\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 1.00000\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 378s 38ms/step - loss: 2.2622 - acc: 0.9822 - val_loss: 2.4854 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 1.00000\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 365s 36ms/step - loss: 2.2307 - acc: 0.9836 - val_loss: 2.2641 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 1.00000\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 356s 36ms/step - loss: 2.2470 - acc: 0.9816 - val_loss: 2.6453 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 1.00000\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 357s 36ms/step - loss: 2.2576 - acc: 0.9826 - val_loss: 2.1799 - val_acc: 0.9588\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 1.00000\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 358s 36ms/step - loss: 2.2410 - acc: 0.9825 - val_loss: 2.2016 - val_acc: 0.9696\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 1.00000\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 364s 36ms/step - loss: 2.2645 - acc: 0.9817 - val_loss: 2.6104 - val_acc: 0.9384\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 1.00000\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 372s 37ms/step - loss: 2.2360 - acc: 0.9831 - val_loss: 2.4622 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 1.00000\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 374s 37ms/step - loss: 2.2342 - acc: 0.9837 - val_loss: 2.3064 - val_acc: 0.9248\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 1.00000\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 381s 38ms/step - loss: 2.2412 - acc: 0.9833 - val_loss: 2.5295 - val_acc: 0.9372\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 1.00000\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 392s 39ms/step - loss: 2.2592 - acc: 0.9832 - val_loss: 2.1281 - val_acc: 0.9924\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 1.00000\n",
      "\n",
      "The top layer trained in time:  4:45:09.126781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/densenet201_using_bottleneck_best.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After we get the bottleneck features, we will build the top fully connected layers on top of the bottlneck features. Let's build the top layers.\n",
    "def train_model_densenet201():\n",
    "    global_start=dt.now()\n",
    "\n",
    "    train_data = np.load('cnn_codes/DenseNet201_bottleneck_features_train.npy')\n",
    "    validation_data = np.load('cnn_codes/DenseNet201_bottleneck_features_test.npy')\n",
    "    \n",
    "    #train_labels = np.array([0] * (nb_train_samples // 3) + [1] * (nb_train_samples // 3) + [2] * (nb_train_samples // 3)) #Equivalent to: np.array([0]*1200 + [1]*1200 + [2]*1200)\n",
    "    #validation_labels = np.array([0] * (nb_validation_samples // 3) + [1] * (nb_validation_samples // 3) + [2] * (nb_validation_samples // 3))\n",
    "    train_labels=generator_tr.classes  \n",
    "    validation_labels=generator_ts.classes\n",
    "    \n",
    "    train_labels = to_categorical(train_labels, num_classes=num_classes)  \n",
    "    validation_labels = to_categorical(validation_labels, num_classes=num_classes)  \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:])) #Ignore the first index. It contains ID\n",
    "\n",
    "    model.add(Dense(256, activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001))) #Best weight initializer for relu is he_normal\n",
    "    model.add(BatchNormalization()) #Add a BatchNormalization layer to control internel covariance shift\n",
    "    model.add(Dropout(rate=0.5)) #Using droput for regularization\n",
    "\n",
    "    model.add(Dense(256, activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001)))\n",
    "    model.add(BatchNormalization()) #Add a BatchNormalization layer to control internel covariance shift\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax',kernel_initializer='glorot_uniform')) #Because we have 3 classes. Remember, softmax is to multi-class, what sigmoid (log reg) is to binary\n",
    "\n",
    "    optim=RMSprop(lr=0.0001, epsilon=1e-8, decay=1e-6)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    #Save the weights for the best epoch accuracy\n",
    "    checkpointer = ModelCheckpoint(filepath=\"weights/densenet201_bottleneck_feats_multi_weights.hdf5\", monitor = 'val_acc',verbose=1, save_best_only=True)\n",
    "                                   \n",
    "    model.fit(x=train_data,\n",
    "              y=train_labels,\n",
    "              epochs=epochs,\n",
    "              validation_data=(validation_data, validation_labels),\n",
    "              callbacks=[checkpointer])    \n",
    "    \n",
    "    #Refit our model with the best weights saved before\n",
    "    model.load_weights('weights/densenet201_bottleneck_feats_multi_weights.hdf5')\n",
    "    model.save('models/dense_net201_botlnck_trained.h5')\n",
    "    print(\"\\nThe top layer trained in time: \",dt.now()-global_start)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model=train_model_densenet201()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Get model performance (DenseNet201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.3363968044281007\n",
      "Validation Accuracy on Unseen Data): 1.0\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('cnn_codes/DenseNet201_bottleneck_features_train.npy')\n",
    "validation_data = np.load('cnn_codes/DenseNet201_bottleneck_features_test.npy')\n",
    "\n",
    "train_labels = to_categorical(generator_tr.classes, num_classes=num_classes)  \n",
    "validation_labels = to_categorical(generator_ts.classes, num_classes=num_classes)\n",
    "\n",
    "#Plot the train and test loss vs number of epochs\n",
    "score = model.evaluate(validation_data, validation_labels, verbose=0) \n",
    "print('Validation Loss:', score[0]) \n",
    "print('Validation Accuracy on Unseen Data):', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Visualize the train and validation loss for DenseNet201 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Get model history\n",
    "history=model.history\n",
    "\n",
    "#Plot train vs test loss\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('Epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
    "\n",
    "#List of epoch numbers\n",
    "x = list(range(1,epochs+1))\n",
    "\n",
    "#Display the loss\n",
    "val_loss = history.history['val_loss'] #Validation Loss\n",
    "loss = history.history['loss'] #Training Loss\n",
    "plt_loss(x, val_loss, loss, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. InceptionResNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Get the bottleneck features using a pre-trained InceptionResNetV2 on Imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 5 classes.\n",
      "Found 2500 images belonging to 5 classes.\n",
      "Got the bottleneck features in time:  1:05:57.233727\n"
     ]
    }
   ],
   "source": [
    "get_bottleneck_features(\"InceptionResNetV2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Train a model with the bottleneck features obtained using InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 55296)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               14156032  \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 14,225,157\n",
      "Trainable params: 14,224,133\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 133s 13ms/step - loss: 24.6721 - acc: 0.9677 - val_loss: 7.4624 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.99520, saving model to weights/InceptionResNetV2_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 129s 13ms/step - loss: 6.1586 - acc: 0.9743 - val_loss: 5.5881 - val_acc: 0.9928\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.99520\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 131s 13ms/step - loss: 5.1651 - acc: 0.9710 - val_loss: 5.0927 - val_acc: 0.9564\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.99520\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 138s 14ms/step - loss: 4.5810 - acc: 0.9701 - val_loss: 4.0252 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.99520\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 137s 14ms/step - loss: 4.1109 - acc: 0.9685 - val_loss: 3.7560 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.99520\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 141s 14ms/step - loss: 3.6189 - acc: 0.9722 - val_loss: 2.9894 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.99520 to 0.99720, saving model to weights/InceptionResNetV2_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 141s 14ms/step - loss: 3.2550 - acc: 0.9736 - val_loss: 2.7532 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.99720\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 142s 14ms/step - loss: 3.0384 - acc: 0.9716 - val_loss: 3.5217 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.99720\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 147s 15ms/step - loss: 2.8304 - acc: 0.9743 - val_loss: 2.3457 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.99720 to 0.99880, saving model to weights/InceptionResNetV2_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 154s 15ms/step - loss: 2.5900 - acc: 0.9752 - val_loss: 2.1742 - val_acc: 0.9996\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.99880 to 0.99960, saving model to weights/InceptionResNetV2_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 158s 16ms/step - loss: 2.4911 - acc: 0.9731 - val_loss: 2.4273 - val_acc: 0.9928\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.99960\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 155s 15ms/step - loss: 2.3117 - acc: 0.9734 - val_loss: 2.7483 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.99960\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 153s 15ms/step - loss: 2.1991 - acc: 0.9739 - val_loss: 2.3443 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.99960\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 151s 15ms/step - loss: 2.1044 - acc: 0.9732 - val_loss: 1.8324 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.99960\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 148s 15ms/step - loss: 1.9766 - acc: 0.9748 - val_loss: 1.7588 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.99960\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 146s 15ms/step - loss: 1.9072 - acc: 0.9752 - val_loss: 1.7293 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.99960\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 146s 15ms/step - loss: 1.8725 - acc: 0.9758 - val_loss: 2.0171 - val_acc: 0.9424\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.99960\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 151s 15ms/step - loss: 1.8342 - acc: 0.9744 - val_loss: 1.4490 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.99960 to 1.00000, saving model to weights/InceptionResNetV2_bottleneck_feats_multi_weights.hdf5\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 149s 15ms/step - loss: 1.7807 - acc: 0.9740 - val_loss: 1.9588 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 1.00000\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 146s 15ms/step - loss: 1.7074 - acc: 0.9757 - val_loss: 1.5627 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 1.00000\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 143s 14ms/step - loss: 1.6305 - acc: 0.9772 - val_loss: 1.8916 - val_acc: 0.9644\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 1.00000\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 141s 14ms/step - loss: 1.6396 - acc: 0.9768 - val_loss: 1.6392 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 1.00000\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 139s 14ms/step - loss: 1.6222 - acc: 0.9759 - val_loss: 1.3917 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 1.00000\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 145s 14ms/step - loss: 1.5704 - acc: 0.9770 - val_loss: 1.3803 - val_acc: 0.9924\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 1.00000\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 148s 15ms/step - loss: 1.5535 - acc: 0.9755 - val_loss: 1.5075 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 1.00000\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 152s 15ms/step - loss: 1.5361 - acc: 0.9783 - val_loss: 1.4452 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 1.00000\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 153s 15ms/step - loss: 1.4990 - acc: 0.9776 - val_loss: 1.4521 - val_acc: 0.9932\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 1.00000\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 148s 15ms/step - loss: 1.4792 - acc: 0.9767 - val_loss: 1.1132 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 1.00000\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 155s 15ms/step - loss: 1.4409 - acc: 0.9790 - val_loss: 1.8507 - val_acc: 0.9432\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 1.00000\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 162s 16ms/step - loss: 1.4731 - acc: 0.9762 - val_loss: 1.4521 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 1.00000\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 161s 16ms/step - loss: 1.4439 - acc: 0.9767 - val_loss: 1.5565 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 1.00000\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 160s 16ms/step - loss: 1.4298 - acc: 0.9778 - val_loss: 1.2849 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 1.00000\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 158s 16ms/step - loss: 1.3995 - acc: 0.9783 - val_loss: 1.5711 - val_acc: 0.9660\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 1.00000\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 156s 16ms/step - loss: 1.4241 - acc: 0.9761 - val_loss: 1.3125 - val_acc: 0.9992\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 1.00000\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 156s 16ms/step - loss: 1.3756 - acc: 0.9796 - val_loss: 1.1043 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 1.00000\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 155s 16ms/step - loss: 1.3697 - acc: 0.9788 - val_loss: 1.4013 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 1.00000\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 154s 15ms/step - loss: 1.4152 - acc: 0.9789 - val_loss: 1.3004 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 1.00000\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 154s 15ms/step - loss: 1.4062 - acc: 0.9771 - val_loss: 1.3983 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 1.00000\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 153s 15ms/step - loss: 1.4094 - acc: 0.9743 - val_loss: 1.3115 - val_acc: 0.9992\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 1.00000\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 150s 15ms/step - loss: 1.3488 - acc: 0.9801 - val_loss: 1.2101 - val_acc: 0.9976\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 1.00000\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 149s 15ms/step - loss: 1.3391 - acc: 0.9796 - val_loss: 1.2271 - val_acc: 0.9908\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 1.00000\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 149s 15ms/step - loss: 1.3346 - acc: 0.9789 - val_loss: 1.3959 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 1.00000\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 154s 15ms/step - loss: 1.3627 - acc: 0.9782 - val_loss: 1.1872 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 1.00000\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 147s 15ms/step - loss: 1.3637 - acc: 0.9784 - val_loss: 1.0999 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 1.00000\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 154s 15ms/step - loss: 1.3457 - acc: 0.9776 - val_loss: 1.1527 - val_acc: 0.9992\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 1.00000\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 162s 16ms/step - loss: 1.3280 - acc: 0.9800 - val_loss: 1.1243 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 1.00000\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 153s 15ms/step - loss: 1.3126 - acc: 0.9801 - val_loss: 1.2799 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 1.00000\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 142s 14ms/step - loss: 1.3266 - acc: 0.9797 - val_loss: 1.2363 - val_acc: 0.9912\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 1.00000\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 144s 14ms/step - loss: 1.3217 - acc: 0.9800 - val_loss: 1.3595 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 1.00000\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 142s 14ms/step - loss: 1.3066 - acc: 0.9802 - val_loss: 1.3808 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 1.00000\n",
      "\n",
      "The top layer trained in time:  2:04:06.505342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/InceptionResNetV2_using_bottleneck_best.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After we get the bottleneck features, we will build the top fully connected layers on top of the bottlneck features. Let's build the top layers.\n",
    "def train_model_InceptionResNetV2():\n",
    "    global_start=dt.now()\n",
    "\n",
    "    train_data = np.load('cnn_codes/InceptionResNetV2_bottleneck_features_train.npy')\n",
    "    validation_data = np.load('cnn_codes/InceptionResNetV2_bottleneck_features_validation.npy')\n",
    "    \n",
    "    #train_labels = np.array([0] * (nb_train_samples // 3) + [1] * (nb_train_samples // 3) + [2] * (nb_train_samples // 3)) #Equivalent to: np.array([0]*1200 + [1]*1200 + [2]*1200)\n",
    "    #validation_labels = np.array([0] * (nb_validation_samples // 3) + [1] * (nb_validation_samples // 3) + [2] * (nb_validation_samples // 3))\n",
    "    train_labels=generator_tr.classes  \n",
    "    validation_labels=generator_ts.classes\n",
    "    \n",
    "    train_labels = to_categorical(train_labels, num_classes=num_classes)  \n",
    "    validation_labels = to_categorical(validation_labels, num_classes=num_classes)  \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:])) #Ignore the first index. It contains ID\n",
    "\n",
    "    model.add(Dense(256, activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001))) #Best weight initializer for relu is he_normal\n",
    "    model.add(BatchNormalization()) #Add a BatchNormalization layer to control internel covariance shift\n",
    "    model.add(Dropout(rate=0.5)) #Using droput for regularization\n",
    "\n",
    "    model.add(Dense(256, activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001)))\n",
    "    model.add(BatchNormalization()) #Add a BatchNormalization layer to control internel covariance shift\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax',kernel_initializer='glorot_uniform')) #Because we have 3 classes. Remember, softmax is to multi-class, what sigmoid (log reg) is to binary\n",
    "\n",
    "    optim=RMSprop(lr=0.0001, epsilon=1e-8, decay=1e-6)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    #Save the weights for the best epoch accuracy\n",
    "    checkpointer = ModelCheckpoint(filepath=\"weights/InceptionResNetV2_bottleneck_feats_multi_weights.hdf5\", monitor = 'val_acc',verbose=1, save_best_only=True)\n",
    "                                   \n",
    "    model.fit(x=train_data,\n",
    "              y=train_labels,\n",
    "              epochs=epochs,\n",
    "              validation_data=(validation_data, validation_labels),\n",
    "              callbacks=[checkpointer])    \n",
    "    \n",
    "    #Refit our model with the best weights saved before\n",
    "    model.load_weights('weights/InceptionResNetV2_bottleneck_feats_multi_weights.hdf5')\n",
    "    model.save('models/incep_res_v2_botlnck_trained.h5')\n",
    "    print(\"\\nThe top layer trained in time: \",dt.now()-global_start)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model=train_model_InceptionResNetV2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Get model performance (InceptionResNetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.44897696723938\n",
      "Validation Accuracy on Unseen Data): 1.0\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('cnn_codes/InceptionResNetV2_bottleneck_features_train.npy')\n",
    "validation_data = np.load('cnn_codes/InceptionResNetV2_bottleneck_features_validation.npy')\n",
    "\n",
    "train_labels = to_categorical(generator_tr.classes, num_classes=num_classes)  \n",
    "validation_labels = to_categorical(generator_ts.classes, num_classes=num_classes)\n",
    "\n",
    "#Plot the train and test loss vs number of epochs\n",
    "score = model.evaluate(validation_data, validation_labels, verbose=0) \n",
    "print('Validation Loss:', score[0]) \n",
    "print('Validation Accuracy on Unseen Data):', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Visualize the train and validation loss for InceptionResNetV2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Get model history\n",
    "history=model.history\n",
    "\n",
    "#Plot train vs test loss\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('Epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
    "\n",
    "#List of epoch numbers\n",
    "x = list(range(1,epochs+1))\n",
    "\n",
    "#Display the loss\n",
    "val_loss = history.history['val_loss'] #Validation Loss\n",
    "loss = history.history['loss'] #Training Loss\n",
    "plt_loss(x, val_loss, loss, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras import applications\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def load_image(file):\n",
    "    test_image = load_img(file, target_size=(256,256))\n",
    "    test_image = img_to_array(test_image)\n",
    "    test_image = test_image / 255.0\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "    return test_image\n",
    "\n",
    "def load_model_trained():\n",
    "    vgg = applications.VGG16(include_top=False, weights='imagenet',input_shape=(256,256,3))\n",
    "    path='models/vgg16_bottleneck_feats_multi_weights.hdf5'\n",
    "    model=load_model(path)\n",
    "    return model, vgg\n",
    "\n",
    "def predict_image(model,vgg,test_image):\n",
    "    start = time.time()\n",
    "    bottleneck_prediction=vgg.predict(test_image)\n",
    "    result = model.predict_classes(bottleneck_prediction) \n",
    "        \n",
    "    \n",
    "    if result[0] == 0:\n",
    "        print(\"It's a Dog!\")\n",
    "    elif result[0] == 1:\n",
    "        print(\"It's a Frog!\")\n",
    "    elif result[0] == 2:\n",
    "        print(\"It's a Giraffe!\")\n",
    "    elif result[0] == 3:\n",
    "        print(\"It's a Horse!\")\n",
    "    elif result[0] == 4:\n",
    "        print(\"It's a Tiger!\")\n",
    "\n",
    "    #Calculate execution time\n",
    "    print(\"\\nTest Time: \",np.round(time.time()-start,2),\"seconds\") \n",
    "    \n",
    "model,vgg=load_model_trained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a Tiger!\n",
      "\n",
      "Test Time:  0.68 seconds\n"
     ]
    }
   ],
   "source": [
    "filename=\"test/006.jpg\"\n",
    "test_image=load_image(filename)\n",
    "\n",
    "predict_image(model,vgg,test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize all the layers in the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('weights/bottleneck_features_train.npy')\n",
    "model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "def visualize_filters(layer, nrows, ncols):\n",
    "    \"\"\"Given a layer as an input, the function will plot all the filters present in that layer in x by y format.\n",
    "    For example suppose our input layer is block1_conv1 and we have 32 feature maps. This function will display\n",
    "    all the 32 feature maps in 8 X 4 view.\"\"\"\n",
    "    filters=layer.get_weights()\n",
    "    fig=plt.figure(figsize=(50,25))\n",
    "    for i in range(0,len(filters)):\n",
    "        ax=fig.add_subplot(ncols,nrows,i+1)\n",
    "        ax.matshow(filters[i][0][0], cmap=matplotlib.cm.binary)\n",
    "        plt.xticks(np.array([]))\n",
    "        plt.yticks(np.array([]))\n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "visualize_filters(model.layers[1],4,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=model.layers[1]\n",
    "filters=layer.get_weights()\n",
    "filters[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\", \n",
    "                               monitor = 'val_acc',\n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "callbacks=[checkpointer] in fit_generator\n",
    "\n",
    "\"\"\"The model will train for 30 epochs but we will use ModelCheckpoint to store the weights of the best performing epoch. \n",
    "We will specify val_acc as the metric to use to define the best model. This means we will keep the weights of the epoch that scores highest in terms of accuracy on the test set.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters[0][3]\n",
    "classifier.load_weights('best_weights.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
