{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import optimizers\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the model weights files.\n",
    "top_model_weights_path = 'bottleneck_feats_multi_weights.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 256, 256\n",
    "\n",
    "#Declaration of parameters needed for training and validation\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 3600 #1200 training samples for each class\n",
    "nb_validation_samples = 1200 #400 training samples for each class\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#Get the VGG16 model trained on ImageNet dataset\n",
    "model = applications.VGG16(weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "#Build layers for a classifier model to be put on top of the convolutional layers\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu',kernel_initializer='he_normal')) #Best weight initializer for relu is he_normal\n",
    "top_model.add(BatchNormalization()) #Add a BatchNormalization layer to control internel covariance shift\n",
    "top_model.add(Dropout(rate=0.5)) #Using droput for regularization\n",
    "\n",
    "top_model.add(Dense(256, activation='relu',kernel_initializer='he_normal'))\n",
    "top_model.add(BatchNormalization()) #Add a BatchNormalization layer to control internel covariance shift\n",
    "top_model.add(Dropout(rate=0.5))\n",
    "top_model.add(Dense(3, activation='softmax')) #Because we have 3 classes. Remember, softmax is to multi-class, what sigmoid (log reg) is to binary\n",
    "\n",
    "#Note that it is necessary to start with a fully-trained classifier, including the top classifier, in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "#Add the model on top of the convolutional weights\n",
    "model.add(top_model)\n",
    "\n",
    "#Set the first few layers (up to the last convolution block) to non-trainable (this means that the weights will not be updated in the CONV layers)\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Compile the model with a SGD/momentum optimizer and a very slow learning rate.\n",
    "#fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer \n",
    "#rather than an adaptative learning rate optimizer such as RMSProp. This is to make sure that the \n",
    "#magnitude of the updates stays very small, so as not to wreck the previously learned features.\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),metrics=['accuracy'])\n",
    "\n",
    "#Get the model summary \n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use the below code snippet for data augmentation on the training data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.4,\n",
    "                                   zoom_range=0.4,\n",
    "                                   vertical_flip=True,\n",
    "                                   rotation_range=30,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "#We won't augment the test data. We will just use ImageDataGenerator to rescale.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                    target_size=(img_width, img_height),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                        target_size=(img_width, img_height),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "#Fine tune the top layers\n",
    "model.fit_generator(train_generator,\n",
    "                    samples_per_epoch=nb_train_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    nb_val_samples=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the model weights files.\n",
    "top_model_weights_path = 'bottleneck_feats_multi_weights.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 256, 256\n",
    "\n",
    "#Declaration of parameters needed for training and validation\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 3600 #1200 training samples for each class\n",
    "nb_validation_samples = 1200 #400 training samples for each class\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#Get the VGG16 model trained on ImageNet dataset\n",
    "model = applications.VGG16(weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "#Build layers for a classifier model to be put on top of the convolutional layers\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu',kernel_initializer='he_normal')) #Best weight initializer for relu is he_normal\n",
    "top_model.add(BatchNormalization()) #Add a BatchNormalization layer to control internel covariance shift\n",
    "top_model.add(Dropout(rate=0.5)) #Using droput for regularization\n",
    "\n",
    "top_model.add(Dense(256, activation='relu',kernel_initializer='he_normal'))\n",
    "top_model.add(BatchNormalization()) #Add a BatchNormalization layer to control internel covariance shift\n",
    "top_model.add(Dropout(rate=0.5))\n",
    "top_model.add(Dense(3, activation='softmax')) #Because we have 3 classes. Remember, softmax is to multi-class, what sigmoid (log reg) is to binary\n",
    "\n",
    "#Note that it is necessary to start with a fully-trained classifier, including the top classifier, in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "#Add the model on top of the convolutional weights\n",
    "model.add(top_model)\n",
    "\n",
    "#Set the first few layers (up to the last convolution block) to non-trainable (this means that the weights will not be updated in the CONV layers)\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Compile the model with a SGD/momentum optimizer and a very slow learning rate.\n",
    "#fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer \n",
    "#rather than an adaptative learning rate optimizer such as RMSProp. This is to make sure that the \n",
    "#magnitude of the updates stays very small, so as not to wreck the previously learned features.\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),metrics=['accuracy'])\n",
    "\n",
    "#Get the model summary \n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
