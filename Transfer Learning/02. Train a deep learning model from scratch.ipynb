{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers as reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 254, 254, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 127, 127, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 125, 125, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 60, 60, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               14745856  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 14,890,115\n",
      "Trainable params: 14,889,091\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Found 3600 images belonging to 3 classes.\n",
      "Found 1200 images belonging to 3 classes.\n",
      "Epoch 1/40\n",
      "225/225 [==============================] - 693s 3s/step - loss: 35.8386 - acc: 0.5108 - val_loss: 21.4433 - val_acc: 0.3742\n",
      "Epoch 2/40\n",
      "225/225 [==============================] - 671s 3s/step - loss: 16.0959 - acc: 0.5758 - val_loss: 14.1809 - val_acc: 0.6142\n",
      "Epoch 3/40\n",
      "225/225 [==============================] - 659s 3s/step - loss: 12.5833 - acc: 0.6331 - val_loss: 12.2545 - val_acc: 0.5983\n",
      "Epoch 4/40\n",
      "225/225 [==============================] - 657s 3s/step - loss: 11.2279 - acc: 0.6475 - val_loss: 10.7201 - val_acc: 0.7192\n",
      "Epoch 5/40\n",
      "225/225 [==============================] - 662s 3s/step - loss: 10.3261 - acc: 0.6867 - val_loss: 9.5953 - val_acc: 0.8008\n",
      "Epoch 6/40\n",
      "225/225 [==============================] - 676s 3s/step - loss: 9.4475 - acc: 0.7158 - val_loss: 8.8109 - val_acc: 0.7717\n",
      "Epoch 7/40\n",
      "225/225 [==============================] - 707s 3s/step - loss: 8.6619 - acc: 0.7503 - val_loss: 8.7618 - val_acc: 0.6267\n",
      "Epoch 8/40\n",
      "225/225 [==============================] - 702s 3s/step - loss: 8.0794 - acc: 0.7636 - val_loss: 7.8814 - val_acc: 0.7400\n",
      "Epoch 9/40\n",
      "225/225 [==============================] - 718s 3s/step - loss: 7.6303 - acc: 0.7614 - val_loss: 8.0039 - val_acc: 0.6767\n",
      "Epoch 10/40\n",
      "225/225 [==============================] - 672s 3s/step - loss: 7.2419 - acc: 0.7717 - val_loss: 7.3232 - val_acc: 0.6783\n",
      "Epoch 11/40\n",
      "225/225 [==============================] - 681s 3s/step - loss: 6.9856 - acc: 0.7775 - val_loss: 8.1943 - val_acc: 0.4658\n",
      "Epoch 12/40\n",
      "225/225 [==============================] - 689s 3s/step - loss: 6.6885 - acc: 0.7828 - val_loss: 6.6140 - val_acc: 0.7308\n",
      "Epoch 13/40\n",
      "225/225 [==============================] - 693s 3s/step - loss: 6.3820 - acc: 0.7814 - val_loss: 6.4408 - val_acc: 0.7192\n",
      "Epoch 14/40\n",
      "225/225 [==============================] - 695s 3s/step - loss: 6.0189 - acc: 0.7964 - val_loss: 5.7679 - val_acc: 0.8092\n",
      "Epoch 15/40\n",
      "225/225 [==============================] - 692s 3s/step - loss: 5.7164 - acc: 0.8006 - val_loss: 5.4295 - val_acc: 0.8117\n",
      "Epoch 16/40\n",
      "225/225 [==============================] - 694s 3s/step - loss: 5.5000 - acc: 0.7978 - val_loss: 5.8230 - val_acc: 0.6808\n",
      "Epoch 17/40\n",
      "225/225 [==============================] - 692s 3s/step - loss: 5.3043 - acc: 0.8108 - val_loss: 5.2846 - val_acc: 0.7458\n",
      "Epoch 18/40\n",
      "225/225 [==============================] - 694s 3s/step - loss: 5.1159 - acc: 0.8083 - val_loss: 4.9616 - val_acc: 0.7983\n",
      "Epoch 19/40\n",
      "225/225 [==============================] - 694s 3s/step - loss: 4.8788 - acc: 0.8189 - val_loss: 4.7661 - val_acc: 0.8092\n",
      "Epoch 20/40\n",
      "225/225 [==============================] - 691s 3s/step - loss: 4.7307 - acc: 0.8194 - val_loss: 4.7283 - val_acc: 0.8117\n",
      "Epoch 21/40\n",
      "225/225 [==============================] - 689s 3s/step - loss: 4.5598 - acc: 0.8297 - val_loss: 5.7494 - val_acc: 0.6583\n",
      "Epoch 22/40\n",
      "225/225 [==============================] - 689s 3s/step - loss: 4.4044 - acc: 0.8211 - val_loss: 4.3529 - val_acc: 0.7992\n",
      "Epoch 23/40\n",
      "225/225 [==============================] - 712s 3s/step - loss: 4.2743 - acc: 0.8344 - val_loss: 4.1772 - val_acc: 0.8408\n",
      "Epoch 24/40\n",
      "225/225 [==============================] - 708s 3s/step - loss: 4.1274 - acc: 0.8406 - val_loss: 4.0125 - val_acc: 0.8542\n",
      "Epoch 25/40\n",
      "225/225 [==============================] - 711s 3s/step - loss: 4.0488 - acc: 0.8297 - val_loss: 3.8858 - val_acc: 0.8658\n",
      "Epoch 26/40\n",
      "225/225 [==============================] - 707s 3s/step - loss: 3.9828 - acc: 0.8311 - val_loss: 5.2406 - val_acc: 0.5325\n",
      "Epoch 27/40\n",
      "225/225 [==============================] - 701s 3s/step - loss: 3.8925 - acc: 0.8347 - val_loss: 3.9280 - val_acc: 0.8100\n",
      "Epoch 28/40\n",
      "225/225 [==============================] - 678s 3s/step - loss: 3.7849 - acc: 0.8406 - val_loss: 3.7565 - val_acc: 0.8617\n",
      "Epoch 29/40\n",
      "225/225 [==============================] - 691s 3s/step - loss: 3.7363 - acc: 0.8356 - val_loss: 4.2528 - val_acc: 0.6575\n",
      "Epoch 30/40\n",
      "225/225 [==============================] - 702s 3s/step - loss: 3.5776 - acc: 0.8447 - val_loss: 3.4659 - val_acc: 0.8742\n",
      "Epoch 31/40\n",
      "225/225 [==============================] - 667s 3s/step - loss: 3.5120 - acc: 0.8403 - val_loss: 3.4625 - val_acc: 0.8525\n",
      "Epoch 32/40\n",
      "225/225 [==============================] - 662s 3s/step - loss: 3.4630 - acc: 0.8406 - val_loss: 3.5593 - val_acc: 0.7917\n",
      "Epoch 33/40\n",
      "225/225 [==============================] - 664s 3s/step - loss: 3.3725 - acc: 0.8419 - val_loss: 3.9037 - val_acc: 0.6050\n",
      "Epoch 34/40\n",
      "225/225 [==============================] - 673s 3s/step - loss: 3.3055 - acc: 0.8447 - val_loss: 3.2005 - val_acc: 0.8558\n",
      "Epoch 35/40\n",
      "225/225 [==============================] - 697s 3s/step - loss: 3.2480 - acc: 0.8483 - val_loss: 3.4218 - val_acc: 0.7600\n",
      "Epoch 36/40\n",
      "225/225 [==============================] - 715s 3s/step - loss: 3.1839 - acc: 0.8489 - val_loss: 3.1752 - val_acc: 0.8358\n",
      "Epoch 37/40\n",
      "225/225 [==============================] - 717s 3s/step - loss: 3.1494 - acc: 0.8508 - val_loss: 3.4426 - val_acc: 0.7350\n",
      "Epoch 38/40\n",
      "225/225 [==============================] - 745s 3s/step - loss: 3.0841 - acc: 0.8567 - val_loss: 3.0823 - val_acc: 0.8450\n",
      "Epoch 39/40\n",
      "162/225 [====================>.........] - ETA: 3:04 - loss: 3.0181 - acc: 0.8611"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "global_start=datetime.now()\n",
    "\n",
    "#Dimensions of our flicker images is 256 X 256\n",
    "img_width, img_height = 256, 256\n",
    "\n",
    "#Declaration of parameters needed for training and validation\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 3600 #1200 training samples for each class\n",
    "nb_validation_samples = 1200 #400 training samples for each class\n",
    "epochs = 40\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "#First convolution layer\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001),input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Second convolution layer\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001),input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Third convolution layer\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001),input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Flatten the outputs of the convolution layer into a 1D contigious array\n",
    "model.add(Flatten())\n",
    "\n",
    "#Add a fully connected layer containing 128 neurons, use activation as relu and a dropout rate of 0.5\n",
    "model.add(Dense(256, activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(rate=0.4))\n",
    "\n",
    "#Add another fully connected layer containing 128 neurons, use activation as relu and a dropout rate of 0.5\n",
    "model.add(Dense(256, activation='relu',kernel_initializer='he_normal',kernel_regularizer=reg.l1_l2(l1=0.001, l2=0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(rate=0.4))\n",
    "\n",
    "#Add the ouput layer containing 3 neurons, because we have 3 categories\n",
    "model.add(Dense(3, activation='softmax',kernel_initializer='glorot_uniform'))\n",
    "\n",
    "optim=RMSprop(lr=0.0001, epsilon=1e-8, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "#We will use the below code snippet for data augmentation on the training data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.4,\n",
    "                                   zoom_range=0.4,\n",
    "                                   vertical_flip=True,\n",
    "                                   rotation_range=30,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "#We won't augment the test data. We will just use ImageDataGenerator to rescale.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                    target_size=(img_width, img_height),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                        target_size=(img_width, img_height),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_validation_samples // batch_size) \n",
    "\n",
    "\n",
    "model.save_weights('cnn_from_scratch_weights.h5') \n",
    "model.save('cnn_from_scratch_model.h5') #Load using: model = load_model('cnn_from_scratch_weights.h5') from keras.models import load_model\n",
    "print(\"Time taken to train the baseline model from scratch: \",datetime.now()-global_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#This function is used to plot/update the train and test loss after each epoch.\n",
    "def plt_dynamic_loss(x, vy, ty, ax, colors=['b']):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n",
    "    ax.plot(x, ty, 'r', label=\"Train Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    fig.canvas.draw()\n",
    "\n",
    "\n",
    "#Get model history\n",
    "history=model.history\n",
    "\n",
    "#Plot train vs test loss\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('Epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
    "\n",
    "#List of epoch numbers\n",
    "x = list(range(1,epochs+1))\n",
    "\n",
    "#Display the loss\n",
    "val_loss = history.history['val_loss'] #Validation Loss\n",
    "loss = history.history['loss'] #Training Loss\n",
    "plt_dynamic_loss(x, val_loss, loss, ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
